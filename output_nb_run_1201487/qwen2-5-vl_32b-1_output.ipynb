{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c95de0f3",
   "metadata": {
    "papermill": {
     "duration": 0.010669,
     "end_time": "2025-07-29T00:09:41.278616",
     "exception": false,
     "start_time": "2025-07-29T00:09:41.267947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# VLM Benchmark for Object Property Abstraction\n",
    "\n",
    "This notebook implements a benchmark for evaluating Vision Language Models (VLMs) on object property abstraction and visual question answering (VQA) tasks. The benchmark includes three types of questions:\n",
    "\n",
    "1. Direct Recognition\n",
    "2. Property Inference\n",
    "3. Counterfactual Reasoning\n",
    "\n",
    "And three types of images:\n",
    "- REAL\n",
    "- ANIMATED\n",
    "- AI GENERATED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0afc675",
   "metadata": {
    "papermill": {
     "duration": 0.005796,
     "end_time": "2025-07-29T00:09:41.290708",
     "exception": false,
     "start_time": "2025-07-29T00:09:41.284912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a86d5e9",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-29T00:09:41.303146Z",
     "iopub.status.busy": "2025-07-29T00:09:41.302811Z",
     "iopub.status.idle": "2025-07-29T00:09:41.306775Z",
     "shell.execute_reply": "2025-07-29T00:09:41.306135Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.011536,
     "end_time": "2025-07-29T00:09:41.307995",
     "exception": false,
     "start_time": "2025-07-29T00:09:41.296459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install transformers torch Pillow tqdm bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e798bb",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-29T00:09:41.320949Z",
     "iopub.status.busy": "2025-07-29T00:09:41.320303Z",
     "iopub.status.idle": "2025-07-29T00:09:43.020025Z",
     "shell.execute_reply": "2025-07-29T00:09:43.019088Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 1.707409,
     "end_time": "2025-07-29T00:09:43.021356",
     "exception": false,
     "start_time": "2025-07-29T00:09:41.313947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qwen-vl-utils in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (0.0.11)\r\n",
      "Requirement already satisfied: flash-attn in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (2.7.4.post1)\r\n",
      "Requirement already satisfied: av in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from qwen-vl-utils) (14.3.0)\r\n",
      "Requirement already satisfied: packaging in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from qwen-vl-utils) (25.0)\r\n",
      "Requirement already satisfied: pillow in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from qwen-vl-utils) (10.3.0)\r\n",
      "Requirement already satisfied: requests in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from qwen-vl-utils) (2.32.3)\r\n",
      "Requirement already satisfied: torch in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from flash-attn) (2.2.1)\r\n",
      "Requirement already satisfied: einops in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from flash-attn) (0.8.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<4,>=2 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from requests->qwen-vl-utils) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from requests->qwen-vl-utils) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from requests->qwen-vl-utils) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from requests->qwen-vl-utils) (2025.4.26)\r\n",
      "Requirement already satisfied: filelock in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (4.13.2)\r\n",
      "Requirement already satisfied: sympy in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.8.93)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from jinja2->torch->flash-attn) (3.0.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from sympy->torch->flash-attn) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install qwen-vl-utils flash-attn #--no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2854cb42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T00:09:43.038039Z",
     "iopub.status.busy": "2025-07-29T00:09:43.037763Z",
     "iopub.status.idle": "2025-07-29T00:09:48.761918Z",
     "shell.execute_reply": "2025-07-29T00:09:48.760993Z"
    },
    "papermill": {
     "duration": 5.732937,
     "end_time": "2025-07-29T00:09:48.763273",
     "exception": false,
     "start_time": "2025-07-29T00:09:43.030336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gc\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada084c",
   "metadata": {
    "papermill": {
     "duration": 0.006616,
     "end_time": "2025-07-29T00:09:48.788275",
     "exception": false,
     "start_time": "2025-07-29T00:09:48.781659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Benchmark Tester Class\n",
    "\n",
    "This class handles the evaluation of models against our benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b085a2d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T00:09:48.802945Z",
     "iopub.status.busy": "2025-07-29T00:09:48.802125Z",
     "iopub.status.idle": "2025-07-29T00:09:48.813175Z",
     "shell.execute_reply": "2025-07-29T00:09:48.812434Z"
    },
    "papermill": {
     "duration": 0.019631,
     "end_time": "2025-07-29T00:09:48.814353",
     "exception": false,
     "start_time": "2025-07-29T00:09:48.794722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class BenchmarkTester:\n",
    "#     def __init__(self, benchmark_path=\"/var/scratch/ave303/OP_bench/benchmark.json\", data_dir=\"/var/scratch/ave303/OP_bench/\"):\n",
    "#         self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         with open(benchmark_path, 'r') as f:\n",
    "#             self.benchmark = json.load(f)\n",
    "#         self.data_dir = data_dir\n",
    "    \n",
    "#     def format_question(self, question, model_name):\n",
    "#         \"\"\"Format a question for the model.\"\"\"\n",
    "\n",
    "#         if model_name==\"blip2\":\n",
    "#             return f\"Question: {question['question']} Answer:\"\n",
    "#         else:\n",
    "#             return f\"Question: {question['question']} Answer with a number and list of objects. Answer:\"\n",
    "\n",
    "#     def clean_answer(self, answer):\n",
    "#         \"\"\"Clean the model output to extract just the number.\"\"\"\n",
    "#         # Remove any text that's not a number\n",
    "#         # import re\n",
    "#         # numbers = re.findall(r'\\d+', answer)\n",
    "#         # if numbers:\n",
    "#         #     return numbers[0]  # Return the first number found\n",
    "#         # return answer\n",
    "#         \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n",
    "#         # Try to extract number and reasoning using regex\n",
    "#         import re\n",
    "#         pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n",
    "#         match = re.search(pattern, answer)\n",
    "        \n",
    "#         if match:\n",
    "#             number = match.group(1)\n",
    "#             objects = [obj.strip() for obj in match.group(2).split(',')]\n",
    "#             return {\n",
    "#                 \"count\": number,\n",
    "#                 \"reasoning\": objects\n",
    "#             }\n",
    "#         else:\n",
    "#             # Fallback if format isn't matched\n",
    "#             numbers = re.findall(r'\\d+', answer)\n",
    "#             return {\n",
    "#                 \"count\": numbers[0] if numbers else \"0\",\n",
    "#                 \"reasoning\": []\n",
    "#             }\n",
    "\n",
    "#     def model_generation(self, model_name, model, inputs, processor):\n",
    "#         \"\"\"Generate answer and decode.\"\"\"\n",
    "#         outputs = None  # Initialize outputs to None\n",
    "        \n",
    "#         if model_name==\"smolVLM2\":\n",
    "#             outputs = model.generate(**inputs, do_sample=False, max_new_tokens=64)\n",
    "#             answer = processor.batch_decode(\n",
    "#                 outputs,\n",
    "#                 skip_special_tokens=True,\n",
    "#             )[0]\n",
    "#         elif model_name==\"Qwen2.5-VL\":\n",
    "#             outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "#             outputs = [\n",
    "#                 out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, outputs)\n",
    "#             ]\n",
    "#             answer = processor.batch_decode(\n",
    "#                 outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "#             )[0]\n",
    "#         else:\n",
    "#             print(f\"Warning: Unknown model name '{model_name}' in model_generation.\")\n",
    "#             answer = \"\"  # Return an empty string\n",
    "\n",
    "#         return answer, outputs\n",
    "    \n",
    "#     def evaluate_model(self, model_name, model, processor, save_path, start_idx=0, batch_size=5):\n",
    "#         results = []\n",
    "#         print(f\"\\nEvaluating {model_name}...\")\n",
    "#         print(f\"Using device: {self.device}\")\n",
    "        \n",
    "#         # Force garbage collection before starting\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         try:\n",
    "#             images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n",
    "#             total_images = len(images)\n",
    "            \n",
    "#             for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n",
    "#                 try:\n",
    "#                     print(f\"\\nProcessing image {idx+1}/{total_images}: {image_data['image_id']}\")\n",
    "#                     image_path = Path(self.data_dir)/image_data['path']\n",
    "#                     if not image_path.exists():\n",
    "#                         print(f\"Warning: Image not found at {image_path}\")\n",
    "#                         continue\n",
    "                    \n",
    "#                     # Load and preprocess image\n",
    "#                     image = Image.open(image_path).convert(\"RGB\")\n",
    "#                     image_results = []  # Store results for current image\n",
    "                    \n",
    "#                     for question in image_data['questions']:\n",
    "#                         try:\n",
    "#                             # prompt = self.format_question(question, model_name)\n",
    "#                             print(f\"Question: {question['question']}\")\n",
    "\n",
    "#                             messages = [\n",
    "#                                 {\n",
    "#                                     \"role\": \"user\",\n",
    "#                                     \"content\": [\n",
    "#                                         {\"type\": \"image\", \"image\": image},\n",
    "#                                         # {\"type\": \"text\", \"text\": f\"{question['question']} Answer format: total number(numerical) objects(within square brackets)\"},\n",
    "#                                         # {\"type\": \"text\", \"text\": f\"{question['question']} Provide just the total count and the list of objects in the given format \\n Format: number [objects]\"},\n",
    "#                                         # {\"type\": \"text\", \"text\": f\"{question['question']} Answer Format: number [objects]\"},\n",
    "#                                         {\"type\": \"text\", \"text\": f\"{question[\"question\"]} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"}\n",
    "#                                     ]\n",
    "#                                 },\n",
    "#                             ]\n",
    "                            \n",
    "#                             # Clear cache before processing each question\n",
    "#                             torch.cuda.empty_cache()\n",
    "                            \n",
    "#                             # Process image and text\n",
    "#                             # inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(self.device)\n",
    "#                             if model_name==\"smolVLM2\":\n",
    "#                                 inputs = processor.apply_chat_template(\n",
    "#                                     messages,\n",
    "#                                     add_generation_prompt=True,\n",
    "#                                     tokenize=True,\n",
    "#                                     return_dict=True,\n",
    "#                                     return_tensors=\"pt\",\n",
    "#                                 ).to(model.device, dtype=torch.float16)\n",
    "#                             else:\n",
    "                                \n",
    "#                                 text = processor.apply_chat_template(\n",
    "#                                     messages, tokenize=False, add_generation_prompt=True\n",
    "#                                 )\n",
    "#                                 # image_inputs, video_inputs = process_vision_info(messages)\n",
    "#                                 inputs = processor(\n",
    "#                                     text=text,\n",
    "#                                     images=image,\n",
    "#                                     videos=None,\n",
    "#                                     padding=True,\n",
    "#                                     return_tensors=\"pt\",\n",
    "#                                 ).to(\"cuda\")\n",
    "                            \n",
    "#                             # Generate answer with better settings\n",
    "#                             with torch.no_grad():\n",
    "#                                 answer, outputs = self.model_generation(model_name, model, inputs, processor)    #call for model.generate\n",
    "        \n",
    "#                             cleaned_answer = self.clean_answer(answer)\n",
    "                            \n",
    "#                             image_results.append({\n",
    "#                                 \"image_id\": image_data[\"image_id\"],\n",
    "#                                 \"image_type\": image_data[\"image_type\"],\n",
    "#                                 \"question_id\": question[\"id\"],\n",
    "#                                 \"question\": question[\"question\"],\n",
    "#                                 \"ground_truth\": question[\"answer\"],\n",
    "#                                 \"model_answer\": cleaned_answer[\"count\"],\n",
    "#                                 \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
    "#                                 \"raw_answer\": answer,  # Keep raw answer for debugging\n",
    "#                                 \"property_category\": question[\"property_category\"]\n",
    "#                             })\n",
    "                            \n",
    "#                             # Clear memory\n",
    "#                             del outputs, inputs\n",
    "#                             torch.cuda.empty_cache()\n",
    "                            \n",
    "#                         except Exception as e:\n",
    "#                             print(f\"Error processing question: {str(e)}\")\n",
    "#                             continue\n",
    "                    \n",
    "#                     # Add results from this image\n",
    "#                     results.extend(image_results)\n",
    "                    \n",
    "#                     # Save intermediate results only every 2 images or if it's the last image\n",
    "#                     if (idx + 1) % 2 == 0 or idx == total_images - 1:\n",
    "#                         with open(f\"{save_path}_checkpoint.json\", 'w') as f:\n",
    "#                             json.dump(results, f, indent=4)\n",
    "                            \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing image {image_data['image_id']}: {str(e)}\")\n",
    "#                     continue\n",
    "            \n",
    "#             # Save final results\n",
    "#             if results:\n",
    "#                 with open(save_path, 'w') as f:\n",
    "#                     json.dump(results, f, indent=4)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred during evaluation: {str(e)}\")\n",
    "#             if results:\n",
    "#                 with open(f\"{save_path}_error_state.json\", 'w') as f:\n",
    "#                     json.dump(results, f, indent=4)\n",
    "        \n",
    "#         return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76321f92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T00:09:48.829244Z",
     "iopub.status.busy": "2025-07-29T00:09:48.828993Z",
     "iopub.status.idle": "2025-07-29T00:09:48.883402Z",
     "shell.execute_reply": "2025-07-29T00:09:48.882642Z"
    },
    "papermill": {
     "duration": 0.06373,
     "end_time": "2025-07-29T00:09:48.884587",
     "exception": false,
     "start_time": "2025-07-29T00:09:48.820857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "class BenchmarkTester:\n",
    "    def __init__(self, benchmark_path=\"/var/scratch/ave303/OP_bench/benchmark.json\", data_dir=\"/var/scratch/ave303/OP_bench/\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        with open(benchmark_path, 'r') as f:\n",
    "            self.benchmark = json.load(f)\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # Set memory optimization environment variables\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "        \n",
    "        # Memory monitoring\n",
    "        self.max_memory_allocated = 0\n",
    "        self.memory_threshold = 0.70  # 70% of GPU memory as threshold\n",
    "\n",
    "    def get_gpu_memory_info(self):\n",
    "        \"\"\"Get current GPU memory usage information.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "            reserved = torch.cuda.memory_reserved() / 1024**3    # GB\n",
    "            max_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB\n",
    "            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n",
    "            \n",
    "            return {\n",
    "                'allocated': allocated,\n",
    "                'reserved': reserved,\n",
    "                'max_allocated': max_memory,\n",
    "                'total': total_memory,\n",
    "                'free': total_memory - allocated,\n",
    "                'usage_percent': (allocated / total_memory) * 100\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    def aggressive_memory_cleanup(self):\n",
    "        \"\"\"Perform aggressive memory cleanup - alias for ultra_aggressive_memory_cleanup.\"\"\"\n",
    "        self.ultra_aggressive_memory_cleanup()\n",
    "\n",
    "    def ultra_aggressive_memory_cleanup(self):\n",
    "        \"\"\"Perform ultra-aggressive memory cleanup including model cache clearing.\"\"\"\n",
    "        # Clear Python garbage collector multiple times\n",
    "        for _ in range(5):\n",
    "            gc.collect()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            # Force synchronize all streams\n",
    "            torch.cuda.synchronize()\n",
    "            # Clear all cached memory\n",
    "            torch.cuda.empty_cache()\n",
    "            # Reset peak memory stats\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            # Force memory defragmentation\n",
    "            torch.cuda.memory.empty_cache()\n",
    "            # Another sync to ensure completion\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "        # Force system memory cleanup\n",
    "        import ctypes\n",
    "        libc = ctypes.CDLL(\"libc.so.6\")\n",
    "        libc.malloc_trim(0)\n",
    "\n",
    "    def check_available_memory_and_restart_if_needed(self):\n",
    "        \"\"\"Check if we need to recommend model restart due to fragmentation.\"\"\"\n",
    "        memory_info = self.get_gpu_memory_info()\n",
    "        if memory_info:\n",
    "            # If allocated is much less than reserved, we have fragmentation\n",
    "            fragmentation_ratio = memory_info['reserved'] / max(memory_info['allocated'], 0.1)\n",
    "            if fragmentation_ratio > 2.0 and memory_info['usage_percent'] > 80:\n",
    "                print(f\"‚ö†Ô∏è  Severe memory fragmentation detected (fragmentation ratio: {fragmentation_ratio:.2f})\")\n",
    "                print(\"Consider restarting the Python process to defragment GPU memory\")\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def resize_image_if_needed(self, image, max_size=(512, 512)):\n",
    "        \"\"\"Resize image aggressively to prevent memory issues.\"\"\"\n",
    "        original_size = image.size\n",
    "        \n",
    "        # Always resize to max_size to ensure consistent memory usage\n",
    "        # Calculate aspect ratio preserving resize\n",
    "        ratio = min(max_size[0] / original_size[0], max_size[1] / original_size[1])\n",
    "        new_size = (int(original_size[0] * ratio), int(original_size[1] * ratio))\n",
    "        \n",
    "        print(f\"Resizing image from {original_size} to {new_size}\")\n",
    "        # Use NEAREST for fastest processing and lowest memory\n",
    "        image = image.resize(new_size, Image.Resampling.NEAREST)\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def check_memory_before_processing(self, image_id, skip_if_high=True):\n",
    "        \"\"\"Check if we have enough memory before processing with option to skip.\"\"\"\n",
    "        memory_info = self.get_gpu_memory_info()\n",
    "        if memory_info and memory_info['usage_percent'] > self.memory_threshold * 100:\n",
    "            print(f\"Warning: High memory usage ({memory_info['usage_percent']:.1f}%) before processing {image_id}\")\n",
    "            self.ultra_aggressive_memory_cleanup()\n",
    "            \n",
    "            # Check again after cleanup\n",
    "            memory_info = self.get_gpu_memory_info()\n",
    "            if memory_info['usage_percent'] > self.memory_threshold * 100:\n",
    "                print(f\"Critical: Still high memory usage ({memory_info['usage_percent']:.1f}%) after cleanup\")\n",
    "                \n",
    "                # Check for fragmentation issues\n",
    "                if not self.check_available_memory_and_restart_if_needed():\n",
    "                    return False\n",
    "                    \n",
    "                if skip_if_high:\n",
    "                    print(f\"Skipping {image_id} due to insufficient memory\")\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def clean_answer(self, answer):\n",
    "        \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n",
    "        import re\n",
    "        pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n",
    "        match = re.search(pattern, answer)\n",
    "        \n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            objects = [obj.strip() for obj in match.group(2).split(',')]\n",
    "            return {\n",
    "                \"count\": number,\n",
    "                \"reasoning\": objects\n",
    "            }\n",
    "        else:\n",
    "            numbers = re.findall(r'\\d+', answer)\n",
    "            return {\n",
    "                \"count\": numbers[0] if numbers else \"0\",\n",
    "                \"reasoning\": []\n",
    "            }\n",
    "\n",
    "    def model_generation(self, model_name, model, inputs, processor):\n",
    "        \"\"\"Generate answer with memory-optimized inference.\"\"\"\n",
    "        outputs = None\n",
    "        \n",
    "        try:\n",
    "            if model_name == \"Qwen2.5-VL\":\n",
    "                # Use gradient checkpointing and mixed precision if available\n",
    "                with torch.cuda.amp.autocast(enabled=True):\n",
    "                    outputs = model.generate(\n",
    "                        **inputs, \n",
    "                        max_new_tokens=200,\n",
    "                        do_sample=False,\n",
    "                        temperature=None,\n",
    "                        top_p=None,\n",
    "                        top_k=None,\n",
    "                        num_beams=1,\n",
    "                        early_stopping=False,\n",
    "                        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                        use_cache=False,  # Disable KV cache to save memory\n",
    "                    )\n",
    "                \n",
    "                outputs = [\n",
    "                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, outputs)\n",
    "                ]\n",
    "                answer = processor.batch_decode(\n",
    "                    outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "                )[0]\n",
    "            else:\n",
    "                print(f\"Warning: Unknown model name '{model_name}' in model_generation.\")\n",
    "                answer = \"\"\n",
    "\n",
    "            return answer, outputs\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            print(f\"CUDA OOM during generation: {e}\")\n",
    "            # Aggressive cleanup and retry once\n",
    "            self.aggressive_memory_cleanup()\n",
    "            raise e\n",
    "\n",
    "    def process_single_question(self, model_name, model, processor, image, question, image_id):\n",
    "        \"\"\"Process a single question with extreme memory optimization.\"\"\"\n",
    "        try:\n",
    "            # Ultra-aggressive pre-check\n",
    "            if not self.check_memory_before_processing(f\"{image_id}_q{question['id']}\", skip_if_high=False):\n",
    "                raise RuntimeError(\"Insufficient GPU memory after cleanup\")\n",
    "\n",
    "            # Create a minimal image copy to avoid references\n",
    "            image_copy = image.copy()\n",
    "            \n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image_copy},\n",
    "                        {\"type\": \"text\", \"text\": f\"{question['question']} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"}\n",
    "                    ]\n",
    "                },\n",
    "            ]\n",
    "            \n",
    "            # Process with maximum memory optimization\n",
    "            text = processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Monitor memory before tokenization\n",
    "            memory_before = self.get_gpu_memory_info()\n",
    "            if memory_before and memory_before['usage_percent'] > 75:\n",
    "                print(f\"‚ö†Ô∏è  Memory usage high before tokenization: {memory_before['usage_percent']:.1f}%\")\n",
    "                self.ultra_aggressive_memory_cleanup()\n",
    "            \n",
    "            # Process inputs with minimal memory footprint\n",
    "            inputs = processor(\n",
    "                text=text,\n",
    "                images=image_copy,\n",
    "                videos=None,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            # Move to device only when needed\n",
    "            inputs = {k: v.to(self.device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "            \n",
    "            # Delete image copy immediately\n",
    "            del image_copy, messages\n",
    "            self.ultra_aggressive_memory_cleanup()\n",
    "            \n",
    "            # Monitor memory before generation\n",
    "            memory_before_gen = self.get_gpu_memory_info()\n",
    "            if memory_before_gen:\n",
    "                print(f\"Memory before generation: {memory_before_gen['usage_percent']:.1f}%\")\n",
    "                if memory_before_gen['usage_percent'] > 85:\n",
    "                    raise RuntimeError(f\"Memory too high for generation: {memory_before_gen['usage_percent']:.1f}%\")\n",
    "            \n",
    "            # Generate with maximum memory efficiency\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):\n",
    "                    answer, outputs = self.model_generation(model_name, model, inputs, processor)\n",
    "            \n",
    "            cleaned_answer = self.clean_answer(answer)\n",
    "            \n",
    "            # Immediate and thorough cleanup\n",
    "            del outputs, inputs\n",
    "            self.ultra_aggressive_memory_cleanup()\n",
    "            \n",
    "            return {\n",
    "                \"question_id\": question[\"id\"],\n",
    "                \"question\": question[\"question\"],\n",
    "                \"ground_truth\": question[\"answer\"],\n",
    "                \"model_answer\": cleaned_answer[\"count\"],\n",
    "                \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
    "                \"raw_answer\": answer,\n",
    "                \"property_category\": question[\"property_category\"]\n",
    "            }\n",
    "            \n",
    "        except (torch.cuda.OutOfMemoryError, RuntimeError) as e:\n",
    "            print(f\"Error processing question {question['id']}: {e}\")\n",
    "            print(\"üß† Retrying with smaller image size and CPU fallback...\")\n",
    "\n",
    "            try:\n",
    "                # Retry with smaller image\n",
    "                smaller_image = self.resize_image_if_needed(image, max_size=(256, 256))\n",
    "\n",
    "                model_cpu = model.to(\"cpu\")\n",
    "                image_copy = smaller_image.copy()\n",
    "\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"image\", \"image\": image_copy},\n",
    "                            {\"type\": \"text\", \"text\": f\"{question['question']} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"}\n",
    "                        ]\n",
    "                    },\n",
    "                ]\n",
    "                text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "                inputs = processor(text=text, images=image_copy, videos=None, padding=True, return_tensors=\"pt\")\n",
    "                answer, outputs = self.model_generation(model_name, model_cpu, inputs, processor)\n",
    "                cleaned_answer = self.clean_answer(answer)\n",
    "\n",
    "                return {\n",
    "                    \"question_id\": question[\"id\"],\n",
    "                    \"question\": question[\"question\"],\n",
    "                    \"ground_truth\": question[\"answer\"],\n",
    "                    \"model_answer\": cleaned_answer[\"count\"],\n",
    "                    \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
    "                    \"raw_answer\": answer,\n",
    "                    \"property_category\": question[\"property_category\"]\n",
    "                }\n",
    "\n",
    "            except Exception as retry_error:\n",
    "                print(f\"‚ö†Ô∏è CPU fallback failed for question {question['id']}: {retry_error}\")\n",
    "                raise retry_error\n",
    "            # print(f\"Error processing question {question['id']}: {e}\")\n",
    "            # self.ultra_aggressive_memory_cleanup()\n",
    "            # raise e\n",
    "\n",
    "    def evaluate_model(self, model_name, model, processor, save_path, start_idx=0, batch_size=5):\n",
    "        results = []\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        successful_images = []\n",
    "        failed_images = []\n",
    "        total_questions_processed = 0\n",
    "        total_questions_failed = 0\n",
    "        \n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initial memory cleanup\n",
    "        self.ultra_aggressive_memory_cleanup()\n",
    "        \n",
    "        # Print initial memory status\n",
    "        memory_info = self.get_gpu_memory_info()\n",
    "        if memory_info:\n",
    "            print(f\"Initial GPU memory: {memory_info['usage_percent']:.1f}% used\")\n",
    "\n",
    "        try:\n",
    "            images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n",
    "            total_images = len(images)\n",
    "            \n",
    "            for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n",
    "                image_start_time = time.time()\n",
    "                current_image_questions_failed = 0\n",
    "                current_image_questions_total = 0\n",
    "                \n",
    "                try:\n",
    "                    image_path = Path(self.data_dir) / image_data['path']\n",
    "                    if not image_path.exists():\n",
    "                        failed_images.append({\n",
    "                            'image_id': image_data['image_id'],\n",
    "                            'image_type': image_data.get('image_type', 'unknown'),\n",
    "                            'error_type': 'file_not_found',\n",
    "                            'error_message': f'Image not found at {image_path}'\n",
    "                        })\n",
    "                        continue\n",
    "                    \n",
    "                    # Load and preprocess image with size control\n",
    "                    image = Image.open(image_path).convert(\"RGB\")\n",
    "                    print(f\"Original image size: {image.size}\")\n",
    "                    \n",
    "                    # Resize aggressively - much smaller images\n",
    "                    image = self.resize_image_if_needed(image, max_size=(384, 384))\n",
    "                    \n",
    "                    image_results = []\n",
    "                    \n",
    "                    # Process questions one by one with memory monitoring\n",
    "                    for question_idx, question in enumerate(image_data['questions']):\n",
    "                        current_image_questions_total += 1\n",
    "                        total_questions_processed += 1\n",
    "                        \n",
    "                        try:\n",
    "                            # Process single question\n",
    "                            question_result = self.process_single_question(\n",
    "                                model_name, model, processor, image, question, image_data['image_id']\n",
    "                            )\n",
    "                            \n",
    "                            # Add image metadata\n",
    "                            question_result.update({\n",
    "                                \"image_id\": image_data[\"image_id\"],\n",
    "                                \"image_type\": image_data.get(\"image_type\", \"unknown\")\n",
    "                            })\n",
    "                            \n",
    "                            image_results.append(question_result)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed question {question['id']}: {e}\")\n",
    "                            current_image_questions_failed += 1\n",
    "                            total_questions_failed += 1\n",
    "                            continue\n",
    "                    \n",
    "                    # Add results from this image\n",
    "                    results.extend(image_results)\n",
    "                    \n",
    "                    # Calculate success metrics\n",
    "                    questions_succeeded = current_image_questions_total - current_image_questions_failed\n",
    "                    \n",
    "                    if current_image_questions_failed == 0:\n",
    "                        successful_images.append({\n",
    "                            'image_id': image_data['image_id'],\n",
    "                            'image_type': image_data.get('image_type', 'unknown'),\n",
    "                            'questions_total': current_image_questions_total,\n",
    "                            'questions_succeeded': questions_succeeded,\n",
    "                            'processing_time': time.time() - image_start_time\n",
    "                        })\n",
    "                    else:\n",
    "                        image_success_rate = (questions_succeeded / current_image_questions_total * 100) if current_image_questions_total > 0 else 0\n",
    "                        failed_images.append({\n",
    "                            'image_id': image_data['image_id'],\n",
    "                            'image_type': image_data.get('image_type', 'unknown'),\n",
    "                            'error_type': 'partial_failure',\n",
    "                            'questions_total': current_image_questions_total,\n",
    "                            'questions_failed': current_image_questions_failed,\n",
    "                            'questions_succeeded': questions_succeeded,\n",
    "                            'success_rate': f\"{image_success_rate:.1f}%\"\n",
    "                        })\n",
    "                    \n",
    "                    # Ultra-aggressive cleanup after each image\n",
    "                    del image\n",
    "                    self.ultra_aggressive_memory_cleanup()\n",
    "                    \n",
    "                    # Save intermediate results\n",
    "                    if (idx + 1) % 2 == 0 or idx == total_images - 1:\n",
    "                        checkpoint_path = f\"{save_path}_checkpoint.json\"\n",
    "                        with open(checkpoint_path, 'w') as f:\n",
    "                            json.dump(results, f, indent=4)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Complete failure for image {image_data['image_id']}: {e}\")\n",
    "                    failed_images.append({\n",
    "                        'image_id': image_data['image_id'],\n",
    "                        'image_type': image_data.get('image_type', 'unknown'),\n",
    "                        'error_type': 'complete_failure',\n",
    "                        'error_message': str(e)\n",
    "                    })\n",
    "                    \n",
    "                    # Cleanup even on failure\n",
    "                    self.ultra_aggressive_memory_cleanup()\n",
    "                    continue\n",
    "            \n",
    "            # Save final results\n",
    "            if results:\n",
    "                with open(save_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Critical error during evaluation: {e}\")\n",
    "            if results:\n",
    "                error_save_path = f\"{save_path}_error_state.json\"\n",
    "                with open(error_save_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "        \n",
    "        # Print comprehensive summary\n",
    "        self._print_evaluation_summary(\n",
    "            model_name, total_images, successful_images, failed_images, \n",
    "            total_questions_processed, total_questions_failed, len(results)\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _print_evaluation_summary(self, model_name, total_images, successful_images, \n",
    "                                failed_images, total_questions_processed, total_questions_failed, total_results):\n",
    "        \"\"\"Print a comprehensive evaluation summary.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EVALUATION SUMMARY FOR {model_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Image-level statistics\n",
    "        num_successful = len(successful_images)\n",
    "        num_failed = len(failed_images)\n",
    "        \n",
    "        print(f\"üìä IMAGE PROCESSING SUMMARY:\")\n",
    "        print(f\"   Total images attempted: {total_images}\")\n",
    "        print(f\"   Successfully processed: {num_successful} ({num_successful/total_images*100:.1f}%)\")\n",
    "        print(f\"   Failed images: {num_failed} ({num_failed/total_images*100:.1f}%)\")\n",
    "        \n",
    "        # Question-level statistics\n",
    "        questions_succeeded = total_questions_processed - total_questions_failed\n",
    "        print(f\"\\nüìù QUESTION PROCESSING SUMMARY:\")\n",
    "        print(f\"   Total questions attempted: {total_questions_processed}\")\n",
    "        print(f\"   Successfully processed: {questions_succeeded} ({questions_succeeded/total_questions_processed*100:.1f}%)\")\n",
    "        print(f\"   Failed questions: {total_questions_failed} ({total_questions_failed/total_questions_processed*100:.1f}%)\")\n",
    "        print(f\"   Results saved: {total_results}\")\n",
    "        \n",
    "        # Memory usage summary\n",
    "        memory_info = self.get_gpu_memory_info()\n",
    "        if memory_info:\n",
    "            print(f\"\\nüß† FINAL MEMORY USAGE:\")\n",
    "            print(f\"   Current allocation: {memory_info['allocated']:.2f} GB ({memory_info['usage_percent']:.1f}%)\")\n",
    "            print(f\"   Peak allocation: {memory_info['max_allocated']:.2f} GB\")\n",
    "            print(f\"   Total GPU memory: {memory_info['total']:.2f} GB\")\n",
    "        \n",
    "        # Successful images details\n",
    "        if successful_images:\n",
    "            print(f\"\\n‚úÖ SUCCESSFUL IMAGES ({len(successful_images)}):\")\n",
    "            for img in successful_images:\n",
    "                print(f\"   ‚Ä¢ {img['image_id']} (Type: {img['image_type']}, \"\n",
    "                      f\"Questions: {img['questions_succeeded']}/{img['questions_total']}, \"\n",
    "                      f\"Time: {img['processing_time']:.1f}s)\")\n",
    "        \n",
    "        # Failed images details\n",
    "        if failed_images:\n",
    "            print(f\"\\n‚ùå FAILED/PROBLEMATIC IMAGES ({len(failed_images)}):\")\n",
    "            for img in failed_images:\n",
    "                if img['error_type'] == 'complete_failure':\n",
    "                    print(f\"   ‚Ä¢ {img['image_id']} (Type: {img['image_type']}) - \"\n",
    "                          f\"COMPLETE FAILURE: {img.get('error_message', 'Unknown error')}\")\n",
    "                elif img['error_type'] == 'partial_failure':\n",
    "                    print(f\"   ‚Ä¢ {img['image_id']} (Type: {img['image_type']}) - \"\n",
    "                          f\"PARTIAL: {img['questions_failed']}/{img['questions_total']} failed \"\n",
    "                          f\"({img['success_rate']} success)\")\n",
    "                elif img['error_type'] == 'file_not_found':\n",
    "                    print(f\"   ‚Ä¢ {img['image_id']} (Type: {img['image_type']}) - \"\n",
    "                          f\"FILE NOT FOUND: {img['error_message']}\")\n",
    "        \n",
    "        print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff2f9a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T00:09:48.899232Z",
     "iopub.status.busy": "2025-07-29T00:09:48.898816Z",
     "iopub.status.idle": "2025-07-29T00:09:48.913657Z",
     "shell.execute_reply": "2025-07-29T00:09:48.912903Z"
    },
    "papermill": {
     "duration": 0.02351,
     "end_time": "2025-07-29T00:09:48.914827",
     "exception": false,
     "start_time": "2025-07-29T00:09:48.891317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class BenchmarkTester:\n",
    "#     def __init__(self, benchmark_path=\"/var/scratch/ave303/OP_bench/benchmark.json\", data_dir=\"/var/scratch/ave303/OP_bench/\"):\n",
    "#         self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         with open(benchmark_path, 'r') as f:\n",
    "#             self.benchmark = json.load(f)\n",
    "#         self.data_dir = data_dir\n",
    "\n",
    "#     def clean_answer(self, answer):\n",
    "#         \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n",
    "#         # Try to extract number and reasoning using regex\n",
    "#         import re\n",
    "#         pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n",
    "#         match = re.search(pattern, answer)\n",
    "        \n",
    "#         if match:\n",
    "#             number = match.group(1)\n",
    "#             objects = [obj.strip() for obj in match.group(2).split(',')]\n",
    "#             return {\n",
    "#                 \"count\": number,\n",
    "#                 \"reasoning\": objects\n",
    "#             }\n",
    "#         else:\n",
    "#             # Fallback if format isn't matched\n",
    "#             numbers = re.findall(r'\\d+', answer)\n",
    "#             return {\n",
    "#                 \"count\": numbers[0] if numbers else \"0\",\n",
    "#                 \"reasoning\": []\n",
    "#             }\n",
    "\n",
    "#     def model_generation(self, model_name, model, inputs, processor):\n",
    "#         \"\"\"Generate answer and decode with greedy decoding.\"\"\"\n",
    "#         outputs = None  # Initialize outputs to None\n",
    "        \n",
    "#         if model_name == \"Qwen2.5-VL\":\n",
    "#             # Explicit greedy decoding parameters\n",
    "#             outputs = model.generate(\n",
    "#                 **inputs, \n",
    "#                 max_new_tokens=200,\n",
    "#                 do_sample=False,          # Disable sampling for greedy decoding\n",
    "#                 temperature=None,         # Not used in greedy decoding\n",
    "#                 top_p=None,              # Not used in greedy decoding  \n",
    "#                 top_k=None,              # Not used in greedy decoding\n",
    "#                 num_beams=1,             # Single beam for greedy decoding\n",
    "#                 early_stopping=False,    # Let it generate until max_tokens or EOS\n",
    "#                 pad_token_id=processor.tokenizer.pad_token_id,\n",
    "#                 eos_token_id=processor.tokenizer.eos_token_id\n",
    "#             )\n",
    "#             outputs = [\n",
    "#                 out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, outputs)\n",
    "#             ]\n",
    "#             answer = processor.batch_decode(\n",
    "#                 outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "#             )[0]\n",
    "#         else:\n",
    "#             print(f\"Warning: Unknown model name '{model_name}' in model_generation.\")\n",
    "#             answer = \"\"  # Return an empty string\n",
    "\n",
    "#         return answer, outputs\n",
    "    \n",
    "#     def evaluate_model(self, model_name, model, processor, save_path, start_idx=0, batch_size=5):\n",
    "#         results = []\n",
    "        \n",
    "#         # Initialize tracking variables\n",
    "#         successful_images = []\n",
    "#         failed_images = []\n",
    "#         total_questions_processed = 0\n",
    "#         total_questions_failed = 0\n",
    "        \n",
    "#         print(f\"\\nEvaluating {model_name}...\")\n",
    "#         print(f\"Using device: {self.device}\")\n",
    "        \n",
    "#         # Force garbage collection before starting\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         try:\n",
    "#             images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n",
    "#             total_images = len(images)\n",
    "            \n",
    "#             for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n",
    "#                 image_start_time = time.time()\n",
    "#                 current_image_questions_failed = 0\n",
    "#                 current_image_questions_total = 0\n",
    "                \n",
    "#                 try:\n",
    "#                     image_path = Path(self.data_dir)/image_data['path']\n",
    "#                     if not image_path.exists():\n",
    "#                         failed_images.append({\n",
    "#                             'image_id': image_data['image_id'],\n",
    "#                             'image_type': image_data.get('image_type', 'unknown'),\n",
    "#                             'error_type': 'file_not_found',\n",
    "#                             'error_message': f'Image not found at {image_path}'\n",
    "#                         })\n",
    "#                         continue\n",
    "                    \n",
    "#                     # Load and preprocess image\n",
    "#                     image = Image.open(image_path).convert(\"RGB\")\n",
    "#                     image_results = []  # Store results for current image\n",
    "                    \n",
    "#                     for question_idx, question in enumerate(image_data['questions']):\n",
    "#                         current_image_questions_total += 1\n",
    "#                         total_questions_processed += 1\n",
    "                        \n",
    "#                         try:\n",
    "#                             messages = [\n",
    "#                                 {\n",
    "#                                     \"role\": \"user\",\n",
    "#                                     \"content\": [\n",
    "#                                         {\"type\": \"image\", \"image\": image},\n",
    "#                                         {\"type\": \"text\", \"text\": f\"{question['question']} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"}\n",
    "#                                     ]\n",
    "#                                 },\n",
    "#                             ]\n",
    "                            \n",
    "#                             # Clear cache before processing each question\n",
    "#                             torch.cuda.empty_cache()\n",
    "                            \n",
    "#                             # Process image and text for Qwen2.5-VL\n",
    "#                             text = processor.apply_chat_template(\n",
    "#                                 messages, tokenize=False, add_generation_prompt=True\n",
    "#                             )\n",
    "#                             inputs = processor(\n",
    "#                                 text=text,\n",
    "#                                 images=image,\n",
    "#                                 videos=None,\n",
    "#                                 padding=True,\n",
    "#                                 return_tensors=\"pt\",\n",
    "#                             ).to(\"cuda\")\n",
    "                            \n",
    "#                             # Generate answer with greedy decoding\n",
    "#                             with torch.no_grad():\n",
    "#                                 answer, outputs = self.model_generation(model_name, model, inputs, processor)\n",
    "        \n",
    "#                             cleaned_answer = self.clean_answer(answer)\n",
    "                            \n",
    "#                             image_results.append({\n",
    "#                                 \"image_id\": image_data[\"image_id\"],\n",
    "#                                 \"image_type\": image_data.get(\"image_type\", \"unknown\"),\n",
    "#                                 \"question_id\": question[\"id\"],\n",
    "#                                 \"question\": question[\"question\"],\n",
    "#                                 \"ground_truth\": question[\"answer\"],\n",
    "#                                 \"model_answer\": cleaned_answer[\"count\"],\n",
    "#                                 \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
    "#                                 \"raw_answer\": answer,  # Keep raw answer for debugging\n",
    "#                                 \"property_category\": question[\"property_category\"]\n",
    "#                             })\n",
    "                            \n",
    "#                             # Clear memory\n",
    "#                             del outputs, inputs\n",
    "#                             torch.cuda.empty_cache()\n",
    "                            \n",
    "#                         except Exception as e:\n",
    "#                             current_image_questions_failed += 1\n",
    "#                             total_questions_failed += 1\n",
    "#                             continue\n",
    "                    \n",
    "#                     # Add results from this image\n",
    "#                     results.extend(image_results)\n",
    "                    \n",
    "#                     # Calculate success rate for this image\n",
    "#                     questions_succeeded = current_image_questions_total - current_image_questions_failed\n",
    "                    \n",
    "#                     if current_image_questions_failed == 0:\n",
    "#                         # All questions succeeded\n",
    "#                         successful_images.append({\n",
    "#                             'image_id': image_data['image_id'],\n",
    "#                             'image_type': image_data.get('image_type', 'unknown'),\n",
    "#                             'questions_total': current_image_questions_total,\n",
    "#                             'questions_succeeded': questions_succeeded,\n",
    "#                             'processing_time': time.time() - image_start_time\n",
    "#                         })\n",
    "#                     else:\n",
    "#                         # Some questions failed\n",
    "#                         image_success_rate = (questions_succeeded / current_image_questions_total * 100) if current_image_questions_total > 0 else 0\n",
    "#                         failed_images.append({\n",
    "#                             'image_id': image_data['image_id'],\n",
    "#                             'image_type': image_data.get('image_type', 'unknown'),\n",
    "#                             'error_type': 'partial_failure',\n",
    "#                             'questions_total': current_image_questions_total,\n",
    "#                             'questions_failed': current_image_questions_failed,\n",
    "#                             'questions_succeeded': questions_succeeded,\n",
    "#                             'success_rate': f\"{image_success_rate:.1f}%\"\n",
    "#                         })\n",
    "                    \n",
    "#                     # Save intermediate results only every 2 images or if it's the last image\n",
    "#                     if (idx + 1) % 2 == 0 or idx == total_images - 1:\n",
    "#                         checkpoint_path = f\"{save_path}_checkpoint.json\"\n",
    "#                         with open(checkpoint_path, 'w') as f:\n",
    "#                             json.dump(results, f, indent=4)\n",
    "                            \n",
    "#                 except Exception as e:\n",
    "#                     failed_images.append({\n",
    "#                         'image_id': image_data['image_id'],\n",
    "#                         'image_type': image_data.get('image_type', 'unknown'),\n",
    "#                         'error_type': 'complete_failure',\n",
    "#                         'error_message': str(e)\n",
    "#                     })\n",
    "#                     continue\n",
    "            \n",
    "#             # Save final results\n",
    "#             if results:\n",
    "#                 with open(save_path, 'w') as f:\n",
    "#                     json.dump(results, f, indent=4)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             if results:\n",
    "#                 error_save_path = f\"{save_path}_error_state.json\"\n",
    "#                 with open(error_save_path, 'w') as f:\n",
    "#                     json.dump(results, f, indent=4)\n",
    "        \n",
    "#         # Print comprehensive summary\n",
    "#         self._print_evaluation_summary(\n",
    "#             model_name, total_images, successful_images, failed_images, \n",
    "#             total_questions_processed, total_questions_failed, len(results)\n",
    "#         )\n",
    "        \n",
    "#         return results\n",
    "    \n",
    "#     def _print_evaluation_summary(self, model_name, total_images, successful_images, \n",
    "#                                 failed_images, total_questions_processed, total_questions_failed, total_results):\n",
    "#         \"\"\"Print a comprehensive evaluation summary.\"\"\"\n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"EVALUATION SUMMARY FOR {model_name.upper()}\")\n",
    "#         print(f\"{'='*60}\")\n",
    "        \n",
    "#         # Image-level statistics\n",
    "#         num_successful = len(successful_images)\n",
    "#         num_failed = len(failed_images)\n",
    "        \n",
    "#         print(f\"üìä IMAGE PROCESSING SUMMARY:\")\n",
    "#         print(f\"   Total images attempted: {total_images}\")\n",
    "#         print(f\"   Successfully processed: {num_successful} ({num_successful/total_images*100:.1f}%)\")\n",
    "#         print(f\"   Failed images: {num_failed} ({num_failed/total_images*100:.1f}%)\")\n",
    "        \n",
    "#         # Question-level statistics\n",
    "#         questions_succeeded = total_questions_processed - total_questions_failed\n",
    "#         print(f\"\\nüìù QUESTION PROCESSING SUMMARY:\")\n",
    "#         print(f\"   Total questions attempted: {total_questions_processed}\")\n",
    "#         print(f\"   Successfully processed: {questions_succeeded} ({questions_succeeded/total_questions_processed*100:.1f}%)\")\n",
    "#         print(f\"   Failed questions: {total_questions_failed} ({total_questions_failed/total_questions_processed*100:.1f}%)\")\n",
    "#         print(f\"   Results saved: {total_results}\")\n",
    "        \n",
    "#         # Successful images details\n",
    "#         if successful_images:\n",
    "#             print(f\"\\n‚úÖ SUCCESSFUL IMAGES ({len(successful_images)}):\")\n",
    "#             for img in successful_images:\n",
    "#                 print(f\"   ‚Ä¢ {img['image_id']} (Type: {img['image_type']}, \"\n",
    "#                       f\"Questions: {img['questions_succeeded']}/{img['questions_total']}, \"\n",
    "#                       f\"Time: {img['processing_time']:.1f}s)\")\n",
    "        \n",
    "#         # Failed images details\n",
    "#         if failed_images:\n",
    "#             print(f\"\\n‚ùå FAILED/PROBLEMATIC IMAGES ({len(failed_images)}):\")\n",
    "#             for img in failed_images:\n",
    "#                 if img['error_type'] == 'complete_failure':\n",
    "#                     print(f\"   ‚Ä¢ {img['image_id']} (Type: {img['image_type']}) - \"\n",
    "#                           f\"COMPLETE FAILURE: {img.get('error_message', 'Unknown error')}\")\n",
    "#                 elif img['error_type'] == 'partial_failure':\n",
    "#                     print(f\"   ‚Ä¢ {img['image_id']} (Type: {img['image_type']}) - \"\n",
    "#                           f\"PARTIAL: {img['questions_failed']}/{img['questions_total']} failed \"\n",
    "#                           f\"({img['success_rate']} success)\")\n",
    "#                 elif img['error_type'] == 'file_not_found':\n",
    "#                     print(f\"   ‚Ä¢ {img['image_id']} (Type: {img['image_type']}) - \"\n",
    "#                           f\"FILE NOT FOUND: {img['error_message']}\")\n",
    "        \n",
    "#         # Group failed images by type\n",
    "#         if failed_images:\n",
    "#             print(f\"\\nüìà FAILURE ANALYSIS BY IMAGE TYPE:\")\n",
    "#             from collections import defaultdict\n",
    "#             failures_by_type = defaultdict(list)\n",
    "#             for img in failed_images:\n",
    "#                 failures_by_type[img['image_type']].append(img)\n",
    "            \n",
    "#             for img_type, failures in failures_by_type.items():\n",
    "#                 print(f\"   ‚Ä¢ {img_type}: {len(failures)} failed images\")\n",
    "#                 for failure in failures:\n",
    "#                     print(f\"     - {failure['image_id']} ({failure['error_type']})\")\n",
    "        \n",
    "#         print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9bb2c",
   "metadata": {
    "papermill": {
     "duration": 0.006269,
     "end_time": "2025-07-29T00:09:48.927560",
     "exception": false,
     "start_time": "2025-07-29T00:09:48.921291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test SmolVLM Model\n",
    "\n",
    "Let's evaluate the SmolVLM2-2.2B-Instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64958f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T00:09:48.941062Z",
     "iopub.status.busy": "2025-07-29T00:09:48.940744Z",
     "iopub.status.idle": "2025-07-29T00:09:48.945051Z",
     "shell.execute_reply": "2025-07-29T00:09:48.944324Z"
    },
    "papermill": {
     "duration": 0.012303,
     "end_time": "2025-07-29T00:09:48.946160",
     "exception": false,
     "start_time": "2025-07-29T00:09:48.933857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def test_smolVLM2():\n",
    "#     from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "#     print(\"Loading smolVLM model...\")\n",
    "    \n",
    "#     model = AutoModelForImageTextToText.from_pretrained(\n",
    "#         \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\",\n",
    "#         torch_dtype=torch.float16,\n",
    "#         attn_implementation=\"flash_attention_2\",\n",
    "#         low_cpu_mem_usage=True,\n",
    "#         trust_remote_code=True\n",
    "#     ).to(\"cuda\")\n",
    "\n",
    "#     processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-2.2B-Instruct\")\n",
    "\n",
    "#     ## A bit slow without the flash_attention2 requires ampere gpu's. Better performance in some cases\n",
    "\n",
    "#     # Optional: Enable memory efficient attention\n",
    "#     if hasattr(model.config, 'use_memory_efficient_attention'):\n",
    "#         model.config.use_memory_efficient_attention = True\n",
    "\n",
    "#     tester = BenchmarkTester()\n",
    "#     smolVLM_results = tester.evaluate_model(\n",
    "#         \"smolVLM2\",\n",
    "#         model, \n",
    "#         processor, \n",
    "#         \"smolVLM2_results_1.json\", \n",
    "#         batch_size=25\n",
    "#     )\n",
    "\n",
    "#     # Clean up\n",
    "#     del model, processor\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c9879",
   "metadata": {
    "papermill": {
     "duration": 0.006099,
     "end_time": "2025-07-29T00:09:48.958619",
     "exception": false,
     "start_time": "2025-07-29T00:09:48.952520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Qwen2.5-VL\n",
    "\n",
    "Lets evaluate the Qwen2.5-VL-7B-Instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed1a2ea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T00:09:48.972681Z",
     "iopub.status.busy": "2025-07-29T00:09:48.972432Z",
     "iopub.status.idle": "2025-07-29T00:09:48.978234Z",
     "shell.execute_reply": "2025-07-29T00:09:48.977560Z"
    },
    "papermill": {
     "duration": 0.014115,
     "end_time": "2025-07-29T00:09:48.979399",
     "exception": false,
     "start_time": "2025-07-29T00:09:48.965284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_Qwen2_5VL():\n",
    "    from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "    \n",
    "    # default: Load the model on the available device(s)\n",
    "    # model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    #     \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    #     load_in_8bit=True, # throws error when .to() is added\n",
    "    #     torch_dtype=torch.bfloat16, \n",
    "    #     device_map=\"auto\",\n",
    "    #     # attn_implementation=\"flash_attention_2\",\n",
    "    #     low_cpu_mem_usage=True\n",
    "    # )\n",
    "    \n",
    "    # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        \"/var/scratch/ave303/models/qwen2-5-vl-32b\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        # attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # default processer\n",
    "    processor = AutoProcessor.from_pretrained(\"/var/scratch/ave303/models/qwen2-5-vl-32b\")\n",
    "\n",
    "    ### Qwen2.5-VL-7B-Instruct --> goes out of CUDA memory\n",
    "    ### Qwen2.5-VL-3B-Instruct --> can handle only 2 images before going out of memory but decent performance\n",
    "\n",
    "    # Optional: Enable memory efficient attention\n",
    "    if hasattr(model.config, 'use_memory_efficient_attention'):\n",
    "        model.config.use_memory_efficient_attention = True\n",
    "\n",
    "    tester = BenchmarkTester()\n",
    "    Qwen2_5VL_results = tester.evaluate_model(\n",
    "        \"Qwen2.5-VL\",\n",
    "        model, \n",
    "        processor, \n",
    "        \"Qwen2.5-VL_32b_results.json\",\n",
    "        start_idx=21,\n",
    "        batch_size=2\n",
    "    )\n",
    "\n",
    "    # Clean up\n",
    "    del model, processor\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b75ee1",
   "metadata": {
    "papermill": {
     "duration": 0.006211,
     "end_time": "2025-07-29T00:09:48.992013",
     "exception": false,
     "start_time": "2025-07-29T00:09:48.985802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Now we can run our evaluation. Let's start with the SmolVLM2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be916cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T00:09:49.005443Z",
     "iopub.status.busy": "2025-07-29T00:09:49.005205Z",
     "iopub.status.idle": "2025-07-29T00:09:49.008659Z",
     "shell.execute_reply": "2025-07-29T00:09:49.008002Z"
    },
    "papermill": {
     "duration": 0.0115,
     "end_time": "2025-07-29T00:09:49.009838",
     "exception": false,
     "start_time": "2025-07-29T00:09:48.998338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_smolVLM2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "621d76a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T00:09:49.023910Z",
     "iopub.status.busy": "2025-07-29T00:09:49.023332Z",
     "iopub.status.idle": "2025-07-29T00:11:48.439024Z",
     "shell.execute_reply": "2025-07-29T00:11:48.438156Z"
    },
    "papermill": {
     "duration": 119.423929,
     "end_time": "2025-07-29T00:11:48.440328",
     "exception": false,
     "start_time": "2025-07-29T00:09:49.016399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   3%|‚ñé         | 1/30 [00:01<00:57,  1.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   7%|‚ñã         | 2/30 [00:04<01:02,  2.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  10%|‚ñà         | 3/30 [00:07<01:08,  2.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  13%|‚ñà‚ñé        | 4/30 [00:10<01:09,  2.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  17%|‚ñà‚ñã        | 5/30 [00:13<01:09,  2.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  20%|‚ñà‚ñà        | 6/30 [00:16<01:08,  2.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  23%|‚ñà‚ñà‚ñé       | 7/30 [00:18<01:04,  2.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  27%|‚ñà‚ñà‚ñã       | 8/30 [00:21<01:00,  2.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  30%|‚ñà‚ñà‚ñà       | 9/30 [00:24<00:59,  2.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 10/30 [00:27<00:57,  2.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  37%|‚ñà‚ñà‚ñà‚ñã      | 11/30 [00:30<00:55,  2.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  40%|‚ñà‚ñà‚ñà‚ñà      | 12/30 [00:33<00:53,  3.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 13/30 [00:36<00:50,  2.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 14/30 [00:39<00:47,  2.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 15/30 [00:42<00:45,  3.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 16/30 [00:45<00:42,  3.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 17/30 [00:48<00:39,  3.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18/30 [00:51<00:36,  3.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 19/30 [00:54<00:32,  2.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 20/30 [00:57<00:28,  2.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 21/30 [00:59<00:25,  2.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 22/30 [01:02<00:21,  2.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 23/30 [01:05<00:18,  2.71s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/30 [01:07<00:16,  2.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 25/30 [01:10<00:13,  2.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 26/30 [01:13<00:10,  2.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 27/30 [01:16<00:08,  2.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 28/30 [01:19<00:05,  2.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 29/30 [01:22<00:02,  2.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:24<00:00,  2.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:24<00:00,  2.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Qwen2.5-VL...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory: 85.8% used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: (4183, 6426)\n",
      "Resizing image from (4183, 6426) to (249, 384)\n",
      "Warning: High memory usage (85.8%) before processing image22_qQ1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical: Still high memory usage (85.8%) after cleanup\n",
      "‚ö†Ô∏è  Memory usage high before tokenization: 85.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before generation: 85.8%\n",
      "Error processing question Q1: Memory too high for generation: 85.8%\n",
      "üß† Retrying with smaller image size and CPU fallback...\n",
      "Resizing image from (249, 384) to (166, 256)\n",
      "‚ö†Ô∏è CPU fallback failed for question Q1: You can't move a model that has some modules offloaded to cpu or disk.\n",
      "Failed question Q1: You can't move a model that has some modules offloaded to cpu or disk.\n",
      "Warning: High memory usage (85.8%) before processing image22_qQ2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical: Still high memory usage (85.8%) after cleanup\n",
      "‚ö†Ô∏è  Memory usage high before tokenization: 85.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before generation: 85.8%\n",
      "Error processing question Q2: Memory too high for generation: 85.8%\n",
      "üß† Retrying with smaller image size and CPU fallback...\n",
      "Resizing image from (249, 384) to (166, 256)\n",
      "‚ö†Ô∏è CPU fallback failed for question Q2: You can't move a model that has some modules offloaded to cpu or disk.\n",
      "Failed question Q2: You can't move a model that has some modules offloaded to cpu or disk.\n",
      "Warning: High memory usage (85.8%) before processing image22_qQ3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical: Still high memory usage (85.8%) after cleanup\n",
      "‚ö†Ô∏è  Memory usage high before tokenization: 85.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before generation: 85.8%\n",
      "Error processing question Q3: Memory too high for generation: 85.8%\n",
      "üß† Retrying with smaller image size and CPU fallback...\n",
      "Resizing image from (249, 384) to (166, 256)\n",
      "‚ö†Ô∏è CPU fallback failed for question Q3: You can't move a model that has some modules offloaded to cpu or disk.\n",
      "Failed question Q3: You can't move a model that has some modules offloaded to cpu or disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:13<00:13, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: (4413, 6619)\n",
      "Resizing image from (4413, 6619) to (256, 384)\n",
      "Warning: High memory usage (85.8%) before processing image23_qQ1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical: Still high memory usage (85.8%) after cleanup\n",
      "‚ö†Ô∏è  Memory usage high before tokenization: 85.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before generation: 85.8%\n",
      "Error processing question Q1: Memory too high for generation: 85.8%\n",
      "üß† Retrying with smaller image size and CPU fallback...\n",
      "Resizing image from (256, 384) to (170, 256)\n",
      "‚ö†Ô∏è CPU fallback failed for question Q1: You can't move a model that has some modules offloaded to cpu or disk.\n",
      "Failed question Q1: You can't move a model that has some modules offloaded to cpu or disk.\n",
      "Warning: High memory usage (85.8%) before processing image23_qQ2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical: Still high memory usage (85.8%) after cleanup\n",
      "‚ö†Ô∏è  Memory usage high before tokenization: 85.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before generation: 85.8%\n",
      "Error processing question Q2: Memory too high for generation: 85.8%\n",
      "üß† Retrying with smaller image size and CPU fallback...\n",
      "Resizing image from (256, 384) to (170, 256)\n",
      "‚ö†Ô∏è CPU fallback failed for question Q2: You can't move a model that has some modules offloaded to cpu or disk.\n",
      "Failed question Q2: You can't move a model that has some modules offloaded to cpu or disk.\n",
      "Warning: High memory usage (85.8%) before processing image23_qQ3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical: Still high memory usage (85.8%) after cleanup\n",
      "‚ö†Ô∏è  Memory usage high before tokenization: 85.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before generation: 85.8%\n",
      "Error processing question Q3: Memory too high for generation: 85.8%\n",
      "üß† Retrying with smaller image size and CPU fallback...\n",
      "Resizing image from (256, 384) to (170, 256)\n",
      "‚ö†Ô∏è CPU fallback failed for question Q3: You can't move a model that has some modules offloaded to cpu or disk.\n",
      "Failed question Q3: You can't move a model that has some modules offloaded to cpu or disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:26<00:00, 13.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:26<00:00, 13.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION SUMMARY FOR QWEN2.5-VL\n",
      "============================================================\n",
      "üìä IMAGE PROCESSING SUMMARY:\n",
      "   Total images attempted: 2\n",
      "   Successfully processed: 0 (0.0%)\n",
      "   Failed images: 2 (100.0%)\n",
      "\n",
      "üìù QUESTION PROCESSING SUMMARY:\n",
      "   Total questions attempted: 6\n",
      "   Successfully processed: 0 (0.0%)\n",
      "   Failed questions: 6 (100.0%)\n",
      "   Results saved: 0\n",
      "\n",
      "üß† FINAL MEMORY USAGE:\n",
      "   Current allocation: 40.88 GB (85.8%)\n",
      "   Peak allocation: 40.88 GB\n",
      "   Total GPU memory: 47.64 GB\n",
      "\n",
      "‚ùå FAILED/PROBLEMATIC IMAGES (2):\n",
      "   ‚Ä¢ image22 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   ‚Ä¢ image23 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_Qwen2_5VL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8aab1",
   "metadata": {
    "papermill": {
     "duration": 0.010767,
     "end_time": "2025-07-29T00:11:48.464919",
     "exception": false,
     "start_time": "2025-07-29T00:11:48.454152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7163706,
     "sourceId": 11436696,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python ((op_bench)",
   "language": "python",
   "name": "op_bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 132.631231,
   "end_time": "2025-07-29T00:11:52.274617",
   "environment_variables": {},
   "exception": null,
   "input_path": "/var/scratch/ave303/OP_bench/opa-benchmark-smolvlm2-qwen2-5-vl.ipynb",
   "output_path": "qwen2-5-vl_32b-1_output.ipynb",
   "parameters": {},
   "start_time": "2025-07-29T00:09:39.643386",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
