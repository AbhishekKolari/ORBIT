{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7c644e",
   "metadata": {
    "papermill": {
     "duration": 0.005931,
     "end_time": "2025-07-28T13:38:08.819123",
     "exception": false,
     "start_time": "2025-07-28T13:38:08.813192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# VLM Benchmark for Object Property Abstraction\n",
    "\n",
    "This notebook implements a benchmark for evaluating Vision Language Models (VLMs) on object property abstraction and visual question answering (VQA) tasks. The benchmark includes three types of questions:\n",
    "\n",
    "1. Direct Recognition\n",
    "2. Property Inference\n",
    "3. Counterfactual Reasoning\n",
    "\n",
    "And three types of images:\n",
    "- REAL\n",
    "- ANIMATED\n",
    "- AI GENERATED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8798913",
   "metadata": {
    "papermill": {
     "duration": 0.004964,
     "end_time": "2025-07-28T13:38:08.829736",
     "exception": false,
     "start_time": "2025-07-28T13:38:08.824772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61fe3070",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-28T13:38:08.840761Z",
     "iopub.status.busy": "2025-07-28T13:38:08.840504Z",
     "iopub.status.idle": "2025-07-28T13:38:08.844582Z",
     "shell.execute_reply": "2025-07-28T13:38:08.843817Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.01112,
     "end_time": "2025-07-28T13:38:08.845908",
     "exception": false,
     "start_time": "2025-07-28T13:38:08.834788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install transformers torch Pillow tqdm bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd01204f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-28T13:38:08.856821Z",
     "iopub.status.busy": "2025-07-28T13:38:08.856573Z",
     "iopub.status.idle": "2025-07-28T13:38:10.554455Z",
     "shell.execute_reply": "2025-07-28T13:38:10.553480Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 1.704915,
     "end_time": "2025-07-28T13:38:10.555860",
     "exception": false,
     "start_time": "2025-07-28T13:38:08.850945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qwen-vl-utils in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (0.0.11)\r\n",
      "Requirement already satisfied: flash-attn in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (2.7.4.post1)\r\n",
      "Requirement already satisfied: av in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from qwen-vl-utils) (14.3.0)\r\n",
      "Requirement already satisfied: packaging in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from qwen-vl-utils) (25.0)\r\n",
      "Requirement already satisfied: pillow in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from qwen-vl-utils) (10.3.0)\r\n",
      "Requirement already satisfied: requests in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from qwen-vl-utils) (2.32.3)\r\n",
      "Requirement already satisfied: torch in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from flash-attn) (2.2.1)\r\n",
      "Requirement already satisfied: einops in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from flash-attn) (0.8.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from requests->qwen-vl-utils) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from requests->qwen-vl-utils) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from requests->qwen-vl-utils) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from requests->qwen-vl-utils) (2025.4.26)\r\n",
      "Requirement already satisfied: filelock in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (4.13.2)\r\n",
      "Requirement already satisfied: sympy in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.8.93)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from jinja2->torch->flash-attn) (3.0.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from sympy->torch->flash-attn) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install qwen-vl-utils flash-attn #--no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf57579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T13:38:10.568726Z",
     "iopub.status.busy": "2025-07-28T13:38:10.568441Z",
     "iopub.status.idle": "2025-07-28T13:38:16.186769Z",
     "shell.execute_reply": "2025-07-28T13:38:16.186120Z"
    },
    "papermill": {
     "duration": 5.625808,
     "end_time": "2025-07-28T13:38:16.187957",
     "exception": false,
     "start_time": "2025-07-28T13:38:10.562149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gc\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee7a15",
   "metadata": {
    "papermill": {
     "duration": 0.005441,
     "end_time": "2025-07-28T13:38:16.199726",
     "exception": false,
     "start_time": "2025-07-28T13:38:16.194285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Benchmark Tester Class\n",
    "\n",
    "This class handles the evaluation of models against our benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7016b805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T13:38:16.211918Z",
     "iopub.status.busy": "2025-07-28T13:38:16.211649Z",
     "iopub.status.idle": "2025-07-28T13:38:16.219223Z",
     "shell.execute_reply": "2025-07-28T13:38:16.218768Z"
    },
    "papermill": {
     "duration": 0.01505,
     "end_time": "2025-07-28T13:38:16.220278",
     "exception": false,
     "start_time": "2025-07-28T13:38:16.205228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class BenchmarkTester:\n",
    "#     def __init__(self, benchmark_path=\"/var/scratch/ave303/OP_bench/benchmark.json\", data_dir=\"/var/scratch/ave303/OP_bench/\"):\n",
    "#         self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         with open(benchmark_path, 'r') as f:\n",
    "#             self.benchmark = json.load(f)\n",
    "#         self.data_dir = data_dir\n",
    "    \n",
    "#     def format_question(self, question, model_name):\n",
    "#         \"\"\"Format a question for the model.\"\"\"\n",
    "\n",
    "#         if model_name==\"blip2\":\n",
    "#             return f\"Question: {question['question']} Answer:\"\n",
    "#         else:\n",
    "#             return f\"Question: {question['question']} Answer with a number and list of objects. Answer:\"\n",
    "\n",
    "#     def clean_answer(self, answer):\n",
    "#         \"\"\"Clean the model output to extract just the number.\"\"\"\n",
    "#         # Remove any text that's not a number\n",
    "#         # import re\n",
    "#         # numbers = re.findall(r'\\d+', answer)\n",
    "#         # if numbers:\n",
    "#         #     return numbers[0]  # Return the first number found\n",
    "#         # return answer\n",
    "#         \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n",
    "#         # Try to extract number and reasoning using regex\n",
    "#         import re\n",
    "#         pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n",
    "#         match = re.search(pattern, answer)\n",
    "        \n",
    "#         if match:\n",
    "#             number = match.group(1)\n",
    "#             objects = [obj.strip() for obj in match.group(2).split(',')]\n",
    "#             return {\n",
    "#                 \"count\": number,\n",
    "#                 \"reasoning\": objects\n",
    "#             }\n",
    "#         else:\n",
    "#             # Fallback if format isn't matched\n",
    "#             numbers = re.findall(r'\\d+', answer)\n",
    "#             return {\n",
    "#                 \"count\": numbers[0] if numbers else \"0\",\n",
    "#                 \"reasoning\": []\n",
    "#             }\n",
    "\n",
    "#     def model_generation(self, model_name, model, inputs, processor):\n",
    "#         \"\"\"Generate answer and decode.\"\"\"\n",
    "#         outputs = None  # Initialize outputs to None\n",
    "        \n",
    "#         if model_name==\"smolVLM2\":\n",
    "#             outputs = model.generate(**inputs, do_sample=False, max_new_tokens=64)\n",
    "#             answer = processor.batch_decode(\n",
    "#                 outputs,\n",
    "#                 skip_special_tokens=True,\n",
    "#             )[0]\n",
    "#         elif model_name==\"Qwen2.5-VL\":\n",
    "#             outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "#             outputs = [\n",
    "#                 out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, outputs)\n",
    "#             ]\n",
    "#             answer = processor.batch_decode(\n",
    "#                 outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "#             )[0]\n",
    "#         else:\n",
    "#             print(f\"Warning: Unknown model name '{model_name}' in model_generation.\")\n",
    "#             answer = \"\"  # Return an empty string\n",
    "\n",
    "#         return answer, outputs\n",
    "    \n",
    "#     def evaluate_model(self, model_name, model, processor, save_path, start_idx=0, batch_size=5):\n",
    "#         results = []\n",
    "#         print(f\"\\nEvaluating {model_name}...\")\n",
    "#         print(f\"Using device: {self.device}\")\n",
    "        \n",
    "#         # Force garbage collection before starting\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         try:\n",
    "#             images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n",
    "#             total_images = len(images)\n",
    "            \n",
    "#             for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n",
    "#                 try:\n",
    "#                     print(f\"\\nProcessing image {idx+1}/{total_images}: {image_data['image_id']}\")\n",
    "#                     image_path = Path(self.data_dir)/image_data['path']\n",
    "#                     if not image_path.exists():\n",
    "#                         print(f\"Warning: Image not found at {image_path}\")\n",
    "#                         continue\n",
    "                    \n",
    "#                     # Load and preprocess image\n",
    "#                     image = Image.open(image_path).convert(\"RGB\")\n",
    "#                     image_results = []  # Store results for current image\n",
    "                    \n",
    "#                     for question in image_data['questions']:\n",
    "#                         try:\n",
    "#                             # prompt = self.format_question(question, model_name)\n",
    "#                             print(f\"Question: {question['question']}\")\n",
    "\n",
    "#                             messages = [\n",
    "#                                 {\n",
    "#                                     \"role\": \"user\",\n",
    "#                                     \"content\": [\n",
    "#                                         {\"type\": \"image\", \"image\": image},\n",
    "#                                         # {\"type\": \"text\", \"text\": f\"{question['question']} Answer format: total number(numerical) objects(within square brackets)\"},\n",
    "#                                         # {\"type\": \"text\", \"text\": f\"{question['question']} Provide just the total count and the list of objects in the given format \\n Format: number [objects]\"},\n",
    "#                                         # {\"type\": \"text\", \"text\": f\"{question['question']} Answer Format: number [objects]\"},\n",
    "#                                         {\"type\": \"text\", \"text\": f\"{question[\"question\"]} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"}\n",
    "#                                     ]\n",
    "#                                 },\n",
    "#                             ]\n",
    "                            \n",
    "#                             # Clear cache before processing each question\n",
    "#                             torch.cuda.empty_cache()\n",
    "                            \n",
    "#                             # Process image and text\n",
    "#                             # inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(self.device)\n",
    "#                             if model_name==\"smolVLM2\":\n",
    "#                                 inputs = processor.apply_chat_template(\n",
    "#                                     messages,\n",
    "#                                     add_generation_prompt=True,\n",
    "#                                     tokenize=True,\n",
    "#                                     return_dict=True,\n",
    "#                                     return_tensors=\"pt\",\n",
    "#                                 ).to(model.device, dtype=torch.float16)\n",
    "#                             else:\n",
    "                                \n",
    "#                                 text = processor.apply_chat_template(\n",
    "#                                     messages, tokenize=False, add_generation_prompt=True\n",
    "#                                 )\n",
    "#                                 # image_inputs, video_inputs = process_vision_info(messages)\n",
    "#                                 inputs = processor(\n",
    "#                                     text=text,\n",
    "#                                     images=image,\n",
    "#                                     videos=None,\n",
    "#                                     padding=True,\n",
    "#                                     return_tensors=\"pt\",\n",
    "#                                 ).to(\"cuda\")\n",
    "                            \n",
    "#                             # Generate answer with better settings\n",
    "#                             with torch.no_grad():\n",
    "#                                 answer, outputs = self.model_generation(model_name, model, inputs, processor)    #call for model.generate\n",
    "        \n",
    "#                             cleaned_answer = self.clean_answer(answer)\n",
    "                            \n",
    "#                             image_results.append({\n",
    "#                                 \"image_id\": image_data[\"image_id\"],\n",
    "#                                 \"image_type\": image_data[\"image_type\"],\n",
    "#                                 \"question_id\": question[\"id\"],\n",
    "#                                 \"question\": question[\"question\"],\n",
    "#                                 \"ground_truth\": question[\"answer\"],\n",
    "#                                 \"model_answer\": cleaned_answer[\"count\"],\n",
    "#                                 \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
    "#                                 \"raw_answer\": answer,  # Keep raw answer for debugging\n",
    "#                                 \"property_category\": question[\"property_category\"]\n",
    "#                             })\n",
    "                            \n",
    "#                             # Clear memory\n",
    "#                             del outputs, inputs\n",
    "#                             torch.cuda.empty_cache()\n",
    "                            \n",
    "#                         except Exception as e:\n",
    "#                             print(f\"Error processing question: {str(e)}\")\n",
    "#                             continue\n",
    "                    \n",
    "#                     # Add results from this image\n",
    "#                     results.extend(image_results)\n",
    "                    \n",
    "#                     # Save intermediate results only every 2 images or if it's the last image\n",
    "#                     if (idx + 1) % 2 == 0 or idx == total_images - 1:\n",
    "#                         with open(f\"{save_path}_checkpoint.json\", 'w') as f:\n",
    "#                             json.dump(results, f, indent=4)\n",
    "                            \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing image {image_data['image_id']}: {str(e)}\")\n",
    "#                     continue\n",
    "            \n",
    "#             # Save final results\n",
    "#             if results:\n",
    "#                 with open(save_path, 'w') as f:\n",
    "#                     json.dump(results, f, indent=4)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred during evaluation: {str(e)}\")\n",
    "#             if results:\n",
    "#                 with open(f\"{save_path}_error_state.json\", 'w') as f:\n",
    "#                     json.dump(results, f, indent=4)\n",
    "        \n",
    "#         return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e92f838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T13:38:16.231942Z",
     "iopub.status.busy": "2025-07-28T13:38:16.231785Z",
     "iopub.status.idle": "2025-07-28T13:38:16.253525Z",
     "shell.execute_reply": "2025-07-28T13:38:16.253019Z"
    },
    "papermill": {
     "duration": 0.028963,
     "end_time": "2025-07-28T13:38:16.254584",
     "exception": false,
     "start_time": "2025-07-28T13:38:16.225621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BenchmarkTester:\n",
    "    def __init__(self, benchmark_path=\"/var/scratch/ave303/OP_bench/benchmark.json\", data_dir=\"/var/scratch/ave303/OP_bench/\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        with open(benchmark_path, 'r') as f:\n",
    "            self.benchmark = json.load(f)\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def clean_answer(self, answer):\n",
    "        \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n",
    "        # Try to extract number and reasoning using regex\n",
    "        import re\n",
    "        pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n",
    "        match = re.search(pattern, answer)\n",
    "        \n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            objects = [obj.strip() for obj in match.group(2).split(',')]\n",
    "            return {\n",
    "                \"count\": number,\n",
    "                \"reasoning\": objects\n",
    "            }\n",
    "        else:\n",
    "            # Fallback if format isn't matched\n",
    "            numbers = re.findall(r'\\d+', answer)\n",
    "            return {\n",
    "                \"count\": numbers[0] if numbers else \"0\",\n",
    "                \"reasoning\": []\n",
    "            }\n",
    "\n",
    "    def model_generation(self, model_name, model, inputs, processor):\n",
    "        \"\"\"Generate answer and decode with greedy decoding.\"\"\"\n",
    "        outputs = None  # Initialize outputs to None\n",
    "        \n",
    "        if model_name == \"Qwen2.5-VL\":\n",
    "            # Explicit greedy decoding parameters\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=200,\n",
    "                do_sample=False,          # Disable sampling for greedy decoding\n",
    "                temperature=None,         # Not used in greedy decoding\n",
    "                top_p=None,              # Not used in greedy decoding  \n",
    "                top_k=None,              # Not used in greedy decoding\n",
    "                num_beams=1,             # Single beam for greedy decoding\n",
    "                early_stopping=False,    # Let it generate until max_tokens or EOS\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "            outputs = [\n",
    "                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, outputs)\n",
    "            ]\n",
    "            answer = processor.batch_decode(\n",
    "                outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "        else:\n",
    "            print(f\"Warning: Unknown model name '{model_name}' in model_generation.\")\n",
    "            answer = \"\"  # Return an empty string\n",
    "\n",
    "        return answer, outputs\n",
    "    \n",
    "    def evaluate_model(self, model_name, model, processor, save_path, start_idx=0, batch_size=5):\n",
    "        results = []\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        successful_images = []\n",
    "        failed_images = []\n",
    "        total_questions_processed = 0\n",
    "        total_questions_failed = 0\n",
    "        \n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Force garbage collection before starting\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        try:\n",
    "            images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n",
    "            total_images = len(images)\n",
    "            \n",
    "            for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n",
    "                image_start_time = time.time()\n",
    "                current_image_questions_failed = 0\n",
    "                current_image_questions_total = 0\n",
    "                \n",
    "                try:\n",
    "                    image_path = Path(self.data_dir)/image_data['path']\n",
    "                    if not image_path.exists():\n",
    "                        failed_images.append({\n",
    "                            'image_id': image_data['image_id'],\n",
    "                            'image_type': image_data.get('image_type', 'unknown'),\n",
    "                            'error_type': 'file_not_found',\n",
    "                            'error_message': f'Image not found at {image_path}'\n",
    "                        })\n",
    "                        continue\n",
    "                    \n",
    "                    # Load and preprocess image\n",
    "                    image = Image.open(image_path).convert(\"RGB\")\n",
    "                    image_results = []  # Store results for current image\n",
    "                    \n",
    "                    for question_idx, question in enumerate(image_data['questions']):\n",
    "                        current_image_questions_total += 1\n",
    "                        total_questions_processed += 1\n",
    "                        \n",
    "                        try:\n",
    "                            messages = [\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": [\n",
    "                                        {\"type\": \"image\", \"image\": image},\n",
    "                                        {\"type\": \"text\", \"text\": f\"{question['question']} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"}\n",
    "                                    ]\n",
    "                                },\n",
    "                            ]\n",
    "                            \n",
    "                            # Clear cache before processing each question\n",
    "                            torch.cuda.empty_cache()\n",
    "                            \n",
    "                            # Process image and text for Qwen2.5-VL\n",
    "                            text = processor.apply_chat_template(\n",
    "                                messages, tokenize=False, add_generation_prompt=True\n",
    "                            )\n",
    "                            inputs = processor(\n",
    "                                text=text,\n",
    "                                images=image,\n",
    "                                videos=None,\n",
    "                                padding=True,\n",
    "                                return_tensors=\"pt\",\n",
    "                            ).to(\"cuda\")\n",
    "                            \n",
    "                            # Generate answer with greedy decoding\n",
    "                            with torch.no_grad():\n",
    "                                answer, outputs = self.model_generation(model_name, model, inputs, processor)\n",
    "        \n",
    "                            cleaned_answer = self.clean_answer(answer)\n",
    "                            \n",
    "                            image_results.append({\n",
    "                                \"image_id\": image_data[\"image_id\"],\n",
    "                                \"image_type\": image_data.get(\"image_type\", \"unknown\"),\n",
    "                                \"question_id\": question[\"id\"],\n",
    "                                \"question\": question[\"question\"],\n",
    "                                \"ground_truth\": question[\"answer\"],\n",
    "                                \"model_answer\": cleaned_answer[\"count\"],\n",
    "                                \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
    "                                \"raw_answer\": answer,  # Keep raw answer for debugging\n",
    "                                \"property_category\": question[\"property_category\"]\n",
    "                            })\n",
    "                            \n",
    "                            # Clear memory\n",
    "                            del outputs, inputs\n",
    "                            torch.cuda.empty_cache()\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            current_image_questions_failed += 1\n",
    "                            total_questions_failed += 1\n",
    "                            continue\n",
    "                    \n",
    "                    # Add results from this image\n",
    "                    results.extend(image_results)\n",
    "                    \n",
    "                    # Calculate success rate for this image\n",
    "                    questions_succeeded = current_image_questions_total - current_image_questions_failed\n",
    "                    \n",
    "                    if current_image_questions_failed == 0:\n",
    "                        # All questions succeeded\n",
    "                        successful_images.append({\n",
    "                            'image_id': image_data['image_id'],\n",
    "                            'image_type': image_data.get('image_type', 'unknown'),\n",
    "                            'questions_total': current_image_questions_total,\n",
    "                            'questions_succeeded': questions_succeeded,\n",
    "                            'processing_time': time.time() - image_start_time\n",
    "                        })\n",
    "                    else:\n",
    "                        # Some questions failed\n",
    "                        image_success_rate = (questions_succeeded / current_image_questions_total * 100) if current_image_questions_total > 0 else 0\n",
    "                        failed_images.append({\n",
    "                            'image_id': image_data['image_id'],\n",
    "                            'image_type': image_data.get('image_type', 'unknown'),\n",
    "                            'error_type': 'partial_failure',\n",
    "                            'questions_total': current_image_questions_total,\n",
    "                            'questions_failed': current_image_questions_failed,\n",
    "                            'questions_succeeded': questions_succeeded,\n",
    "                            'success_rate': f\"{image_success_rate:.1f}%\"\n",
    "                        })\n",
    "                    \n",
    "                    # Save intermediate results only every 2 images or if it's the last image\n",
    "                    if (idx + 1) % 2 == 0 or idx == total_images - 1:\n",
    "                        checkpoint_path = f\"{save_path}_checkpoint.json\"\n",
    "                        with open(checkpoint_path, 'w') as f:\n",
    "                            json.dump(results, f, indent=4)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    failed_images.append({\n",
    "                        'image_id': image_data['image_id'],\n",
    "                        'image_type': image_data.get('image_type', 'unknown'),\n",
    "                        'error_type': 'complete_failure',\n",
    "                        'error_message': str(e)\n",
    "                    })\n",
    "                    continue\n",
    "            \n",
    "            # Save final results\n",
    "            if results:\n",
    "                with open(save_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "            \n",
    "        except Exception as e:\n",
    "            if results:\n",
    "                error_save_path = f\"{save_path}_error_state.json\"\n",
    "                with open(error_save_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "        \n",
    "        # Print comprehensive summary\n",
    "        self._print_evaluation_summary(\n",
    "            model_name, total_images, successful_images, failed_images, \n",
    "            total_questions_processed, total_questions_failed, len(results)\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _print_evaluation_summary(self, model_name, total_images, successful_images, \n",
    "                                failed_images, total_questions_processed, total_questions_failed, total_results):\n",
    "        \"\"\"Print a comprehensive evaluation summary.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EVALUATION SUMMARY FOR {model_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Image-level statistics\n",
    "        num_successful = len(successful_images)\n",
    "        num_failed = len(failed_images)\n",
    "        \n",
    "        print(f\"ðŸ“Š IMAGE PROCESSING SUMMARY:\")\n",
    "        print(f\"   Total images attempted: {total_images}\")\n",
    "        print(f\"   Successfully processed: {num_successful} ({num_successful/total_images*100:.1f}%)\")\n",
    "        print(f\"   Failed images: {num_failed} ({num_failed/total_images*100:.1f}%)\")\n",
    "        \n",
    "        # Question-level statistics\n",
    "        questions_succeeded = total_questions_processed - total_questions_failed\n",
    "        print(f\"\\nðŸ“ QUESTION PROCESSING SUMMARY:\")\n",
    "        print(f\"   Total questions attempted: {total_questions_processed}\")\n",
    "        print(f\"   Successfully processed: {questions_succeeded} ({questions_succeeded/total_questions_processed*100:.1f}%)\")\n",
    "        print(f\"   Failed questions: {total_questions_failed} ({total_questions_failed/total_questions_processed*100:.1f}%)\")\n",
    "        print(f\"   Results saved: {total_results}\")\n",
    "        \n",
    "        # Successful images details\n",
    "        if successful_images:\n",
    "            print(f\"\\nâœ… SUCCESSFUL IMAGES ({len(successful_images)}):\")\n",
    "            for img in successful_images:\n",
    "                print(f\"   â€¢ {img['image_id']} (Type: {img['image_type']}, \"\n",
    "                      f\"Questions: {img['questions_succeeded']}/{img['questions_total']}, \"\n",
    "                      f\"Time: {img['processing_time']:.1f}s)\")\n",
    "        \n",
    "        # Failed images details\n",
    "        if failed_images:\n",
    "            print(f\"\\nâŒ FAILED/PROBLEMATIC IMAGES ({len(failed_images)}):\")\n",
    "            for img in failed_images:\n",
    "                if img['error_type'] == 'complete_failure':\n",
    "                    print(f\"   â€¢ {img['image_id']} (Type: {img['image_type']}) - \"\n",
    "                          f\"COMPLETE FAILURE: {img.get('error_message', 'Unknown error')}\")\n",
    "                elif img['error_type'] == 'partial_failure':\n",
    "                    print(f\"   â€¢ {img['image_id']} (Type: {img['image_type']}) - \"\n",
    "                          f\"PARTIAL: {img['questions_failed']}/{img['questions_total']} failed \"\n",
    "                          f\"({img['success_rate']} success)\")\n",
    "                elif img['error_type'] == 'file_not_found':\n",
    "                    print(f\"   â€¢ {img['image_id']} (Type: {img['image_type']}) - \"\n",
    "                          f\"FILE NOT FOUND: {img['error_message']}\")\n",
    "        \n",
    "        # Group failed images by type\n",
    "        if failed_images:\n",
    "            print(f\"\\nðŸ“ˆ FAILURE ANALYSIS BY IMAGE TYPE:\")\n",
    "            from collections import defaultdict\n",
    "            failures_by_type = defaultdict(list)\n",
    "            for img in failed_images:\n",
    "                failures_by_type[img['image_type']].append(img)\n",
    "            \n",
    "            for img_type, failures in failures_by_type.items():\n",
    "                print(f\"   â€¢ {img_type}: {len(failures)} failed images\")\n",
    "                for failure in failures:\n",
    "                    print(f\"     - {failure['image_id']} ({failure['error_type']})\")\n",
    "        \n",
    "        print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53d3a1",
   "metadata": {
    "papermill": {
     "duration": 0.005178,
     "end_time": "2025-07-28T13:38:16.265186",
     "exception": false,
     "start_time": "2025-07-28T13:38:16.260008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test SmolVLM Model\n",
    "\n",
    "Let's evaluate the SmolVLM2-2.2B-Instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "040ff973",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T13:38:16.276552Z",
     "iopub.status.busy": "2025-07-28T13:38:16.276385Z",
     "iopub.status.idle": "2025-07-28T13:38:16.279731Z",
     "shell.execute_reply": "2025-07-28T13:38:16.279263Z"
    },
    "papermill": {
     "duration": 0.010251,
     "end_time": "2025-07-28T13:38:16.280823",
     "exception": false,
     "start_time": "2025-07-28T13:38:16.270572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def test_smolVLM2():\n",
    "#     from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "#     print(\"Loading smolVLM model...\")\n",
    "    \n",
    "#     model = AutoModelForImageTextToText.from_pretrained(\n",
    "#         \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\",\n",
    "#         torch_dtype=torch.float16,\n",
    "#         attn_implementation=\"flash_attention_2\",\n",
    "#         low_cpu_mem_usage=True,\n",
    "#         trust_remote_code=True\n",
    "#     ).to(\"cuda\")\n",
    "\n",
    "#     processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-2.2B-Instruct\")\n",
    "\n",
    "#     ## A bit slow without the flash_attention2 requires ampere gpu's. Better performance in some cases\n",
    "\n",
    "#     # Optional: Enable memory efficient attention\n",
    "#     if hasattr(model.config, 'use_memory_efficient_attention'):\n",
    "#         model.config.use_memory_efficient_attention = True\n",
    "\n",
    "#     tester = BenchmarkTester()\n",
    "#     smolVLM_results = tester.evaluate_model(\n",
    "#         \"smolVLM2\",\n",
    "#         model, \n",
    "#         processor, \n",
    "#         \"smolVLM2_results_1.json\", \n",
    "#         batch_size=25\n",
    "#     )\n",
    "\n",
    "#     # Clean up\n",
    "#     del model, processor\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721cb0ef",
   "metadata": {
    "papermill": {
     "duration": 0.005182,
     "end_time": "2025-07-28T13:38:16.291380",
     "exception": false,
     "start_time": "2025-07-28T13:38:16.286198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Qwen2.5-VL\n",
    "\n",
    "Lets evaluate the Qwen2.5-VL-7B-Instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a8e042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T13:38:16.303071Z",
     "iopub.status.busy": "2025-07-28T13:38:16.302834Z",
     "iopub.status.idle": "2025-07-28T13:38:16.307165Z",
     "shell.execute_reply": "2025-07-28T13:38:16.306712Z"
    },
    "papermill": {
     "duration": 0.011279,
     "end_time": "2025-07-28T13:38:16.308293",
     "exception": false,
     "start_time": "2025-07-28T13:38:16.297014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_Qwen2_5VL():\n",
    "    from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "    \n",
    "    # default: Load the model on the available device(s)\n",
    "    # model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    #     \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    #     load_in_8bit=True, # throws error when .to() is added\n",
    "    #     torch_dtype=torch.bfloat16, \n",
    "    #     device_map=\"auto\",\n",
    "    #     # attn_implementation=\"flash_attention_2\",\n",
    "    #     low_cpu_mem_usage=True\n",
    "    # )\n",
    "    \n",
    "    # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        \"/var/scratch/ave303/models/qwen2-5-vl-7b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        # attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # default processer\n",
    "    processor = AutoProcessor.from_pretrained(\"/var/scratch/ave303/models/qwen2-5-vl-7b\")\n",
    "\n",
    "    ### Qwen2.5-VL-7B-Instruct --> goes out of CUDA memory\n",
    "    ### Qwen2.5-VL-3B-Instruct --> can handle only 2 images before going out of memory but decent performance\n",
    "\n",
    "    # Optional: Enable memory efficient attention\n",
    "    if hasattr(model.config, 'use_memory_efficient_attention'):\n",
    "        model.config.use_memory_efficient_attention = True\n",
    "\n",
    "    tester = BenchmarkTester()\n",
    "    Qwen2_5VL_results = tester.evaluate_model(\n",
    "        \"Qwen2.5-VL\",\n",
    "        model, \n",
    "        processor, \n",
    "        \"Qwen2.5-VL_7b_results.json\",\n",
    "        # start_idx=2,\n",
    "        batch_size=360\n",
    "    )\n",
    "\n",
    "    # Clean up\n",
    "    del model, processor\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3bb5b7",
   "metadata": {
    "papermill": {
     "duration": 0.005228,
     "end_time": "2025-07-28T13:38:16.318868",
     "exception": false,
     "start_time": "2025-07-28T13:38:16.313640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Now we can run our evaluation. Let's start with the SmolVLM2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bb7d6b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T13:38:16.330246Z",
     "iopub.status.busy": "2025-07-28T13:38:16.330009Z",
     "iopub.status.idle": "2025-07-28T13:38:16.332796Z",
     "shell.execute_reply": "2025-07-28T13:38:16.332333Z"
    },
    "papermill": {
     "duration": 0.009608,
     "end_time": "2025-07-28T13:38:16.333845",
     "exception": false,
     "start_time": "2025-07-28T13:38:16.324237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_smolVLM2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cb357c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T13:38:16.345172Z",
     "iopub.status.busy": "2025-07-28T13:38:16.344938Z",
     "iopub.status.idle": "2025-07-28T15:07:20.334309Z",
     "shell.execute_reply": "2025-07-28T15:07:20.333673Z"
    },
    "papermill": {
     "duration": 5343.999438,
     "end_time": "2025-07-28T15:07:20.338574",
     "exception": false,
     "start_time": "2025-07-28T13:38:16.339136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  14%|â–ˆâ–        | 1/7 [00:04<00:29,  4.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:09<00:24,  4.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:14<00:19,  4.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:19<00:14,  4.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:23<00:09,  4.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:28<00:04,  4.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:31<00:00,  4.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:31<00:00,  4.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Qwen2.5-VL...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   0%|          | 0/360 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   0%|          | 1/360 [00:07<43:57,  7.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   1%|          | 2/360 [00:09<25:49,  4.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   1%|          | 3/360 [00:21<45:11,  7.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   1%|          | 4/360 [00:22<30:59,  5.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   1%|â–         | 5/360 [00:30<35:44,  6.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   2%|â–         | 6/360 [00:35<34:39,  5.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   2%|â–         | 7/360 [00:37<26:03,  4.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   2%|â–         | 8/360 [00:38<19:34,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   2%|â–Ž         | 9/360 [00:39<16:13,  2.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   3%|â–Ž         | 10/360 [00:41<14:54,  2.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   3%|â–Ž         | 11/360 [00:46<18:55,  3.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   3%|â–Ž         | 12/360 [00:49<18:00,  3.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   4%|â–Ž         | 13/360 [00:50<15:05,  2.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   4%|â–         | 14/360 [00:52<13:46,  2.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   4%|â–         | 15/360 [01:02<26:03,  4.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   4%|â–         | 16/360 [01:15<40:56,  7.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   5%|â–         | 17/360 [01:16<30:58,  5.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   5%|â–Œ         | 18/360 [01:18<24:23,  4.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   5%|â–Œ         | 19/360 [01:20<20:53,  3.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   6%|â–Œ         | 20/360 [01:25<22:25,  3.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   6%|â–Œ         | 21/360 [01:27<19:31,  3.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   6%|â–Œ         | 22/360 [03:05<2:59:32, 31.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   6%|â–‹         | 23/360 [04:42<4:48:13, 51.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   7%|â–‹         | 24/360 [04:44<3:24:22, 36.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   7%|â–‹         | 25/360 [04:45<2:24:30, 25.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   7%|â–‹         | 26/360 [04:47<1:44:44, 18.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   8%|â–Š         | 27/360 [04:49<1:15:58, 13.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   8%|â–Š         | 28/360 [04:51<55:32, 10.04s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   8%|â–Š         | 29/360 [04:52<41:21,  7.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   8%|â–Š         | 30/360 [04:55<33:33,  6.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   9%|â–Š         | 31/360 [04:58<28:31,  5.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   9%|â–‰         | 32/360 [05:00<23:26,  4.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   9%|â–‰         | 33/360 [05:04<22:01,  4.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   9%|â–‰         | 34/360 [05:05<18:18,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  10%|â–‰         | 35/360 [05:08<16:22,  3.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  10%|â–ˆ         | 36/360 [05:09<14:18,  2.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  10%|â–ˆ         | 37/360 [05:12<14:38,  2.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  11%|â–ˆ         | 38/360 [05:17<17:07,  3.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  11%|â–ˆ         | 39/360 [05:19<16:01,  3.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  11%|â–ˆ         | 40/360 [05:23<17:50,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  11%|â–ˆâ–        | 41/360 [05:25<14:48,  2.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  12%|â–ˆâ–        | 42/360 [05:28<15:27,  2.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  12%|â–ˆâ–        | 43/360 [05:31<15:26,  2.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  12%|â–ˆâ–        | 44/360 [05:39<23:30,  4.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  12%|â–ˆâ–Ž        | 45/360 [05:44<24:24,  4.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  13%|â–ˆâ–Ž        | 46/360 [05:46<20:31,  3.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  13%|â–ˆâ–Ž        | 47/360 [05:49<18:44,  3.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  13%|â–ˆâ–Ž        | 48/360 [05:51<15:30,  2.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  14%|â–ˆâ–Ž        | 49/360 [05:54<15:49,  3.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  14%|â–ˆâ–        | 50/360 [06:02<23:01,  4.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  14%|â–ˆâ–        | 51/360 [06:08<26:16,  5.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  14%|â–ˆâ–        | 52/360 [06:10<21:43,  4.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  15%|â–ˆâ–        | 53/360 [06:16<22:55,  4.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  15%|â–ˆâ–Œ        | 54/360 [07:52<2:42:57, 31.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  15%|â–ˆâ–Œ        | 55/360 [08:06<2:15:13, 26.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  16%|â–ˆâ–Œ        | 56/360 [09:42<4:00:38, 47.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  16%|â–ˆâ–Œ        | 57/360 [10:01<3:17:19, 39.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  16%|â–ˆâ–Œ        | 58/360 [10:06<2:24:15, 28.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  16%|â–ˆâ–‹        | 59/360 [10:10<1:47:27, 21.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  17%|â–ˆâ–‹        | 60/360 [10:27<1:40:37, 20.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  17%|â–ˆâ–‹        | 61/360 [10:41<1:30:57, 18.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  17%|â–ˆâ–‹        | 62/360 [10:49<1:15:02, 15.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  18%|â–ˆâ–Š        | 63/360 [11:03<1:13:06, 14.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  18%|â–ˆâ–Š        | 64/360 [11:29<1:29:33, 18.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  18%|â–ˆâ–Š        | 65/360 [11:31<1:05:29, 13.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  18%|â–ˆâ–Š        | 66/360 [11:33<48:59, 10.00s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  19%|â–ˆâ–Š        | 67/360 [11:41<46:05,  9.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  19%|â–ˆâ–‰        | 68/360 [11:49<43:19,  8.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  19%|â–ˆâ–‰        | 69/360 [11:57<41:36,  8.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  19%|â–ˆâ–‰        | 70/360 [12:13<51:54, 10.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  20%|â–ˆâ–‰        | 71/360 [12:38<1:12:13, 14.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  20%|â–ˆâ–ˆ        | 72/360 [12:45<1:01:31, 12.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  20%|â–ˆâ–ˆ        | 73/360 [12:54<55:58, 11.70s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  21%|â–ˆâ–ˆ        | 74/360 [13:00<46:37,  9.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  21%|â–ˆâ–ˆ        | 75/360 [14:35<2:48:37, 35.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  21%|â–ˆâ–ˆ        | 76/360 [14:43<2:08:53, 27.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  21%|â–ˆâ–ˆâ–       | 77/360 [14:45<1:32:33, 19.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  22%|â–ˆâ–ˆâ–       | 78/360 [14:53<1:16:15, 16.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  22%|â–ˆâ–ˆâ–       | 79/360 [15:02<1:04:37, 13.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  22%|â–ˆâ–ˆâ–       | 80/360 [15:09<56:07, 12.03s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  22%|â–ˆâ–ˆâ–Ž       | 81/360 [15:17<49:45, 10.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  23%|â–ˆâ–ˆâ–Ž       | 82/360 [15:36<1:00:28, 13.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  23%|â–ˆâ–ˆâ–Ž       | 83/360 [15:37<44:22,  9.61s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  23%|â–ˆâ–ˆâ–Ž       | 84/360 [16:51<2:12:34, 28.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  24%|â–ˆâ–ˆâ–Ž       | 85/360 [17:22<2:16:02, 29.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  24%|â–ˆâ–ˆâ–       | 86/360 [17:24<1:37:28, 21.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  24%|â–ˆâ–ˆâ–       | 87/360 [17:32<1:18:40, 17.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  24%|â–ˆâ–ˆâ–       | 88/360 [17:40<1:05:47, 14.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  25%|â–ˆâ–ˆâ–       | 89/360 [18:05<1:19:09, 17.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  25%|â–ˆâ–ˆâ–Œ       | 90/360 [18:29<1:27:41, 19.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  25%|â–ˆâ–ˆâ–Œ       | 91/360 [18:32<1:05:11, 14.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  26%|â–ˆâ–ˆâ–Œ       | 92/360 [20:09<2:55:45, 39.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  26%|â–ˆâ–ˆâ–Œ       | 93/360 [20:17<2:13:08, 29.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  26%|â–ˆâ–ˆâ–Œ       | 94/360 [20:26<1:44:31, 23.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  26%|â–ˆâ–ˆâ–‹       | 95/360 [20:34<1:23:07, 18.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  27%|â–ˆâ–ˆâ–‹       | 96/360 [20:54<1:25:01, 19.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  27%|â–ˆâ–ˆâ–‹       | 97/360 [21:19<1:32:32, 21.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  27%|â–ˆâ–ˆâ–‹       | 98/360 [21:27<1:14:37, 17.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  28%|â–ˆâ–ˆâ–Š       | 99/360 [22:41<2:28:30, 34.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  28%|â–ˆâ–ˆâ–Š       | 100/360 [22:49<1:53:58, 26.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  28%|â–ˆâ–ˆâ–Š       | 101/360 [22:57<1:30:03, 20.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  28%|â–ˆâ–ˆâ–Š       | 102/360 [23:06<1:14:25, 17.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  29%|â–ˆâ–ˆâ–Š       | 103/360 [23:16<1:04:10, 14.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  29%|â–ˆâ–ˆâ–‰       | 104/360 [23:23<54:44, 12.83s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  29%|â–ˆâ–ˆâ–‰       | 105/360 [23:29<44:36, 10.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  29%|â–ˆâ–ˆâ–‰       | 106/360 [23:47<55:07, 13.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  30%|â–ˆâ–ˆâ–‰       | 107/360 [23:55<48:30, 11.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  30%|â–ˆâ–ˆâ–ˆ       | 108/360 [24:21<1:05:57, 15.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  30%|â–ˆâ–ˆâ–ˆ       | 109/360 [24:47<1:18:14, 18.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  31%|â–ˆâ–ˆâ–ˆ       | 110/360 [25:12<1:26:16, 20.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  31%|â–ˆâ–ˆâ–ˆ       | 111/360 [25:37<1:31:39, 22.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  31%|â–ˆâ–ˆâ–ˆ       | 112/360 [25:47<1:15:20, 18.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  31%|â–ˆâ–ˆâ–ˆâ–      | 113/360 [25:48<54:45, 13.30s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  32%|â–ˆâ–ˆâ–ˆâ–      | 114/360 [25:55<46:37, 11.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  32%|â–ˆâ–ˆâ–ˆâ–      | 115/360 [26:04<43:47, 10.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  32%|â–ˆâ–ˆâ–ˆâ–      | 116/360 [26:30<1:01:17, 15.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 117/360 [26:36<50:47, 12.54s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 118/360 [26:44<44:57, 11.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 119/360 [26:51<39:42,  9.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 120/360 [27:00<37:58,  9.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 121/360 [27:01<28:05,  7.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  34%|â–ˆâ–ˆâ–ˆâ–      | 122/360 [27:02<20:58,  5.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  34%|â–ˆâ–ˆâ–ˆâ–      | 123/360 [27:04<17:02,  4.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  34%|â–ˆâ–ˆâ–ˆâ–      | 124/360 [27:08<16:15,  4.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  35%|â–ˆâ–ˆâ–ˆâ–      | 125/360 [27:10<13:52,  3.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 126/360 [27:12<12:20,  3.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 127/360 [27:14<10:49,  2.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 128/360 [27:18<12:04,  3.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/360 [27:21<11:31,  2.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/360 [28:58<1:59:30, 31.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 131/360 [29:20<1:48:46, 28.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 132/360 [29:36<1:33:46, 24.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 133/360 [30:00<1:33:09, 24.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/360 [30:02<1:06:59, 17.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 135/360 [30:30<1:18:04, 20.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 136/360 [30:58<1:25:10, 22.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 137/360 [31:02<1:04:35, 17.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/360 [31:05<47:51, 12.94s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 139/360 [31:08<36:38,  9.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 140/360 [31:12<29:47,  8.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 141/360 [32:48<2:06:21, 34.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 142/360 [33:03<1:44:49, 28.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 143/360 [34:41<2:58:25, 49.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144/360 [34:43<2:06:33, 35.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 145/360 [34:45<1:30:34, 25.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 146/360 [34:47<1:05:36, 18.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/360 [34:49<47:52, 13.49s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/360 [34:52<35:44, 10.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 149/360 [34:53<26:52,  7.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 150/360 [34:55<20:12,  5.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 151/360 [34:57<16:11,  4.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/360 [34:58<12:53,  3.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 153/360 [35:00<10:57,  3.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 154/360 [35:02<09:42,  2.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 155/360 [35:04<08:54,  2.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 156/360 [35:07<09:08,  2.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 157/360 [35:09<08:27,  2.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/360 [35:11<08:01,  2.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/360 [35:14<08:06,  2.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/360 [35:16<07:43,  2.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/360 [36:51<1:40:16, 30.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 162/360 [37:19<1:37:06, 29.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 163/360 [38:57<2:44:25, 50.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 164/360 [40:34<3:28:53, 63.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 165/360 [42:09<3:58:08, 73.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/360 [43:50<4:23:47, 81.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 167/360 [44:17<3:30:30, 65.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 168/360 [44:45<2:52:58, 54.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 169/360 [45:12<2:26:42, 46.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/360 [45:28<1:56:39, 36.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 171/360 [47:08<2:56:29, 56.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 172/360 [48:46<3:34:58, 68.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 173/360 [50:22<3:59:20, 76.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 174/360 [50:24<2:48:25, 54.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/360 [50:25<1:58:14, 38.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 176/360 [50:26<1:23:12, 27.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 177/360 [50:27<59:04, 19.37s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 178/360 [52:05<2:09:57, 42.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/360 [53:43<2:59:05, 59.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 180/360 [53:44<2:06:03, 42.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 181/360 [53:47<1:30:18, 30.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 182/360 [53:52<1:07:07, 22.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 183/360 [53:57<51:05, 17.32s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/360 [53:58<36:23, 12.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 185/360 [54:02<29:03,  9.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 186/360 [54:07<24:08,  8.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 187/360 [54:14<22:46,  7.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188/360 [54:19<20:43,  7.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 189/360 [54:25<19:30,  6.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 190/360 [54:32<19:23,  6.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 191/360 [54:38<18:08,  6.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 192/360 [54:43<17:16,  6.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 193/360 [54:49<17:13,  6.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/360 [54:54<16:10,  5.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/360 [55:00<15:45,  5.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/360 [55:06<15:58,  5.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/360 [55:11<15:15,  5.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 198/360 [55:16<14:50,  5.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 199/360 [55:23<15:22,  5.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 200/360 [55:28<15:14,  5.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 201/360 [55:31<12:39,  4.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/360 [55:33<10:20,  3.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 203/360 [55:35<08:43,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 204/360 [57:12<1:22:05, 31.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 205/360 [57:15<58:57, 22.83s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 206/360 [57:17<42:30, 16.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 207/360 [57:19<31:01, 12.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 208/360 [57:20<22:35,  8.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 209/360 [57:21<16:51,  6.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 210/360 [57:23<12:42,  5.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 211/360 [57:24<10:01,  4.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 212/360 [57:26<08:05,  3.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 213/360 [57:27<06:44,  2.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 214/360 [57:29<05:51,  2.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 215/360 [57:30<04:59,  2.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 216/360 [57:32<04:46,  1.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 217/360 [57:43<11:27,  4.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 218/360 [57:47<10:11,  4.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 219/360 [57:49<08:47,  3.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 220/360 [57:50<06:55,  2.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 221/360 [57:55<07:59,  3.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 222/360 [57:56<06:33,  2.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223/360 [57:58<06:05,  2.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224/360 [58:01<05:50,  2.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 225/360 [58:02<05:15,  2.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 226/360 [58:05<05:12,  2.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 227/360 [58:06<04:40,  2.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 228/360 [58:08<04:08,  1.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 229/360 [58:13<06:01,  2.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/360 [59:49<1:07:03, 30.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/360 [1:01:31<1:51:51, 52.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/360 [1:03:08<2:20:05, 65.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/360 [1:04:47<2:40:18, 75.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 234/360 [1:05:15<2:09:00, 61.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 235/360 [1:06:55<2:31:54, 72.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 236/360 [1:08:32<2:45:20, 80.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 237/360 [1:10:08<2:54:00, 84.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 238/360 [1:11:45<3:00:24, 88.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 239/360 [1:13:23<3:04:17, 91.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 240/360 [1:15:01<3:06:59, 93.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 241/360 [1:15:05<2:11:43, 66.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 242/360 [1:15:08<1:33:18, 47.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 243/360 [1:15:11<1:06:36, 34.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 244/360 [1:15:16<48:59, 25.34s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 245/360 [1:15:19<35:37, 18.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 246/360 [1:15:21<26:19, 13.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 247/360 [1:15:26<21:02, 11.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 248/360 [1:15:32<17:36,  9.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 249/360 [1:15:36<14:49,  8.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 250/360 [1:15:40<12:10,  6.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 251/360 [1:15:44<10:56,  6.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 252/360 [1:15:49<09:46,  5.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 253/360 [1:15:53<09:17,  5.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 254/360 [1:15:55<07:35,  4.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 255/360 [1:15:58<06:35,  3.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 256/360 [1:16:00<05:47,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 257/360 [1:16:03<05:10,  3.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258/360 [1:16:08<06:11,  3.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259/360 [1:16:10<05:24,  3.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260/360 [1:16:12<04:48,  2.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 261/360 [1:16:14<04:18,  2.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 262/360 [1:16:16<04:00,  2.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 263/360 [1:16:21<05:08,  3.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 264/360 [1:16:24<04:53,  3.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 265/360 [1:16:26<04:20,  2.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/360 [1:16:31<05:17,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/360 [1:16:47<11:14,  7.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/360 [1:16:49<08:52,  5.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/360 [1:16:51<06:49,  4.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 270/360 [1:16:53<05:32,  3.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 271/360 [1:16:54<04:36,  3.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 272/360 [1:16:56<04:02,  2.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 273/360 [1:16:59<03:48,  2.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 274/360 [1:17:01<03:37,  2.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 275/360 [1:17:03<03:20,  2.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 276/360 [1:17:05<03:14,  2.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 277/360 [1:17:07<03:05,  2.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 278/360 [1:17:10<03:12,  2.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 279/360 [1:17:26<08:41,  6.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 280/360 [1:17:28<06:57,  5.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 281/360 [1:17:30<05:43,  4.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 282/360 [1:17:33<04:54,  3.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 283/360 [1:17:49<09:37,  7.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 284/360 [1:17:52<07:57,  6.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 285/360 [1:17:55<06:17,  5.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 286/360 [1:17:57<05:14,  4.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 287/360 [1:17:59<04:33,  3.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 288/360 [1:18:02<03:59,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 289/360 [1:18:18<08:32,  7.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 290/360 [1:18:33<10:57,  9.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 291/360 [1:18:49<13:21, 11.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 292/360 [1:19:06<14:54, 13.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 293/360 [1:19:23<15:57, 14.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294/360 [1:19:40<16:34, 15.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295/360 [1:19:44<12:38, 11.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296/360 [1:19:48<09:56,  9.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 297/360 [1:19:53<08:30,  8.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 298/360 [1:19:58<07:32,  7.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 299/360 [1:20:03<06:42,  6.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 300/360 [1:20:08<06:01,  6.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 301/360 [1:20:24<08:59,  9.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/360 [1:20:41<11:04, 11.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/360 [1:20:58<12:24, 13.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/360 [1:21:14<13:05, 14.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/360 [1:21:30<13:26, 14.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 306/360 [1:21:48<13:58, 15.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 307/360 [1:22:04<13:57, 15.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 308/360 [1:22:21<13:49, 15.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 309/360 [1:22:37<13:40, 16.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 310/360 [1:22:54<13:32, 16.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 311/360 [1:22:56<09:44, 11.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 312/360 [1:22:58<07:11,  8.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 313/360 [1:23:00<05:27,  6.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 314/360 [1:23:02<04:14,  5.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 315/360 [1:23:04<03:24,  4.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 316/360 [1:23:07<02:48,  3.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 317/360 [1:23:10<02:37,  3.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 318/360 [1:23:13<02:32,  3.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 319/360 [1:23:16<02:22,  3.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 320/360 [1:23:20<02:22,  3.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 321/360 [1:23:24<02:17,  3.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 322/360 [1:23:27<02:11,  3.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 323/360 [1:23:43<04:29,  7.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 324/360 [1:23:47<03:40,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 325/360 [1:23:50<03:01,  5.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 326/360 [1:23:53<02:37,  4.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 327/360 [1:24:09<04:25,  8.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 328/360 [1:24:25<05:33, 10.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329/360 [1:24:28<04:14,  8.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330/360 [1:24:31<03:20,  6.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331/360 [1:24:47<04:35,  9.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332/360 [1:24:50<03:32,  7.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 333/360 [1:24:54<02:49,  6.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 334/360 [1:24:57<02:20,  5.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 335/360 [1:25:00<02:00,  4.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 336/360 [1:25:17<03:17,  8.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 337/360 [1:25:20<02:33,  6.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/360 [1:25:35<03:27,  9.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/360 [1:25:39<02:38,  7.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/360 [1:25:55<03:24, 10.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/360 [1:26:12<03:49, 12.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 342/360 [1:26:15<02:49,  9.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 343/360 [1:26:18<02:07,  7.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 344/360 [1:26:21<01:41,  6.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 345/360 [1:26:25<01:20,  5.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 346/360 [1:26:41<02:00,  8.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 347/360 [1:26:57<02:22, 10.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 348/360 [1:27:00<01:43,  8.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 349/360 [1:27:03<01:16,  6.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 350/360 [1:27:20<01:37,  9.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 351/360 [1:27:23<01:09,  7.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 352/360 [1:27:26<00:50,  6.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 353/360 [1:27:29<00:37,  5.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 354/360 [1:27:32<00:28,  4.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 355/360 [1:27:34<00:20,  4.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 356/360 [1:27:37<00:14,  3.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 357/360 [1:27:41<00:10,  3.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 358/360 [1:27:57<00:14,  7.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 359/360 [1:28:14<00:10, 10.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [1:28:27<00:00, 11.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [1:28:27<00:00, 14.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION SUMMARY FOR QWEN2.5-VL\n",
      "============================================================\n",
      "ðŸ“Š IMAGE PROCESSING SUMMARY:\n",
      "   Total images attempted: 360\n",
      "   Successfully processed: 360 (100.0%)\n",
      "   Failed images: 0 (0.0%)\n",
      "\n",
      "ðŸ“ QUESTION PROCESSING SUMMARY:\n",
      "   Total questions attempted: 1080\n",
      "   Successfully processed: 1080 (100.0%)\n",
      "   Failed questions: 0 (0.0%)\n",
      "   Results saved: 1080\n",
      "\n",
      "âœ… SUCCESSFUL IMAGES (360):\n",
      "   â€¢ image01 (Type: REAL, Questions: 3/3, Time: 7.3s)\n",
      "   â€¢ image02 (Type: REAL, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image03 (Type: REAL, Questions: 3/3, Time: 11.5s)\n",
      "   â€¢ image04 (Type: REAL, Questions: 3/3, Time: 1.6s)\n",
      "   â€¢ image05 (Type: REAL, Questions: 3/3, Time: 7.5s)\n",
      "   â€¢ image06 (Type: REAL, Questions: 3/3, Time: 5.5s)\n",
      "   â€¢ image07 (Type: REAL, Questions: 3/3, Time: 1.5s)\n",
      "   â€¢ image08 (Type: REAL, Questions: 3/3, Time: 1.0s)\n",
      "   â€¢ image09 (Type: REAL, Questions: 3/3, Time: 1.5s)\n",
      "   â€¢ image10 (Type: REAL, Questions: 3/3, Time: 2.1s)\n",
      "   â€¢ image11 (Type: REAL, Questions: 3/3, Time: 4.8s)\n",
      "   â€¢ image12 (Type: REAL, Questions: 3/3, Time: 2.8s)\n",
      "   â€¢ image13 (Type: REAL, Questions: 3/3, Time: 1.5s)\n",
      "   â€¢ image14 (Type: REAL, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image15 (Type: REAL, Questions: 3/3, Time: 9.5s)\n",
      "   â€¢ image16 (Type: REAL, Questions: 3/3, Time: 13.2s)\n",
      "   â€¢ image17 (Type: REAL, Questions: 3/3, Time: 1.4s)\n",
      "   â€¢ image18 (Type: REAL, Questions: 3/3, Time: 1.6s)\n",
      "   â€¢ image19 (Type: REAL, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image20 (Type: REAL, Questions: 3/3, Time: 4.6s)\n",
      "   â€¢ image21 (Type: REAL, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image22 (Type: REAL, Questions: 3/3, Time: 97.9s)\n",
      "   â€¢ image23 (Type: REAL, Questions: 3/3, Time: 96.7s)\n",
      "   â€¢ image24 (Type: REAL, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image25 (Type: REAL, Questions: 3/3, Time: 1.1s)\n",
      "   â€¢ image26 (Type: REAL, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image27 (Type: REAL, Questions: 3/3, Time: 1.7s)\n",
      "   â€¢ image28 (Type: REAL, Questions: 3/3, Time: 1.5s)\n",
      "   â€¢ image29 (Type: REAL, Questions: 3/3, Time: 1.6s)\n",
      "   â€¢ image30 (Type: REAL, Questions: 3/3, Time: 2.8s)\n",
      "   â€¢ image31 (Type: REAL, Questions: 3/3, Time: 3.1s)\n",
      "   â€¢ image32 (Type: REAL, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image33 (Type: REAL, Questions: 3/3, Time: 3.5s)\n",
      "   â€¢ image34 (Type: REAL, Questions: 3/3, Time: 1.8s)\n",
      "   â€¢ image35 (Type: REAL, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image36 (Type: REAL, Questions: 3/3, Time: 1.8s)\n",
      "   â€¢ image37 (Type: REAL, Questions: 3/3, Time: 2.9s)\n",
      "   â€¢ image38 (Type: REAL, Questions: 3/3, Time: 4.3s)\n",
      "   â€¢ image39 (Type: REAL, Questions: 3/3, Time: 2.5s)\n",
      "   â€¢ image40 (Type: REAL, Questions: 3/3, Time: 4.2s)\n",
      "   â€¢ image41 (Type: REAL, Questions: 3/3, Time: 1.5s)\n",
      "   â€¢ image42 (Type: REAL, Questions: 3/3, Time: 3.2s)\n",
      "   â€¢ image43 (Type: REAL, Questions: 3/3, Time: 2.9s)\n",
      "   â€¢ image44 (Type: REAL, Questions: 3/3, Time: 8.1s)\n",
      "   â€¢ image45 (Type: REAL, Questions: 3/3, Time: 5.1s)\n",
      "   â€¢ image46 (Type: REAL, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image47 (Type: REAL, Questions: 3/3, Time: 2.8s)\n",
      "   â€¢ image48 (Type: REAL, Questions: 3/3, Time: 1.6s)\n",
      "   â€¢ image49 (Type: REAL, Questions: 3/3, Time: 3.2s)\n",
      "   â€¢ image50 (Type: REAL, Questions: 3/3, Time: 7.7s)\n",
      "   â€¢ image51 (Type: REAL, Questions: 3/3, Time: 6.6s)\n",
      "   â€¢ image52 (Type: REAL, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image53 (Type: REAL, Questions: 3/3, Time: 5.1s)\n",
      "   â€¢ image54 (Type: REAL, Questions: 3/3, Time: 96.0s)\n",
      "   â€¢ image55 (Type: REAL, Questions: 3/3, Time: 14.1s)\n",
      "   â€¢ image56 (Type: REAL, Questions: 3/3, Time: 96.2s)\n",
      "   â€¢ image57 (Type: REAL, Questions: 3/3, Time: 19.4s)\n",
      "   â€¢ image58 (Type: REAL, Questions: 3/3, Time: 4.4s)\n",
      "   â€¢ image59 (Type: REAL, Questions: 3/3, Time: 4.5s)\n",
      "   â€¢ image60 (Type: REAL, Questions: 3/3, Time: 17.1s)\n",
      "   â€¢ image61 (Type: REAL, Questions: 3/3, Time: 13.9s)\n",
      "   â€¢ image62 (Type: REAL, Questions: 3/3, Time: 7.8s)\n",
      "   â€¢ image63 (Type: REAL, Questions: 3/3, Time: 14.0s)\n",
      "   â€¢ image64 (Type: REAL, Questions: 3/3, Time: 26.0s)\n",
      "   â€¢ image65 (Type: REAL, Questions: 3/3, Time: 2.0s)\n",
      "   â€¢ image66 (Type: REAL, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image67 (Type: REAL, Questions: 3/3, Time: 8.1s)\n",
      "   â€¢ image68 (Type: REAL, Questions: 3/3, Time: 7.6s)\n",
      "   â€¢ image69 (Type: REAL, Questions: 3/3, Time: 7.8s)\n",
      "   â€¢ image70 (Type: REAL, Questions: 3/3, Time: 15.8s)\n",
      "   â€¢ image71 (Type: REAL, Questions: 3/3, Time: 24.9s)\n",
      "   â€¢ image72 (Type: REAL, Questions: 3/3, Time: 7.7s)\n",
      "   â€¢ image73 (Type: REAL, Questions: 3/3, Time: 9.1s)\n",
      "   â€¢ image74 (Type: REAL, Questions: 3/3, Time: 5.3s)\n",
      "   â€¢ image75 (Type: REAL, Questions: 3/3, Time: 95.5s)\n",
      "   â€¢ image76 (Type: REAL, Questions: 3/3, Time: 7.9s)\n",
      "   â€¢ image77 (Type: REAL, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image78 (Type: REAL, Questions: 3/3, Time: 8.3s)\n",
      "   â€¢ image79 (Type: REAL, Questions: 3/3, Time: 8.1s)\n",
      "   â€¢ image80 (Type: REAL, Questions: 3/3, Time: 7.9s)\n",
      "   â€¢ image81 (Type: REAL, Questions: 3/3, Time: 7.6s)\n",
      "   â€¢ image82 (Type: REAL, Questions: 3/3, Time: 18.5s)\n",
      "   â€¢ image83 (Type: REAL, Questions: 3/3, Time: 1.6s)\n",
      "   â€¢ image84 (Type: REAL, Questions: 3/3, Time: 73.6s)\n",
      "   â€¢ image85 (Type: REAL, Questions: 3/3, Time: 31.7s)\n",
      "   â€¢ image86 (Type: REAL, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image87 (Type: REAL, Questions: 3/3, Time: 7.8s)\n",
      "   â€¢ image88 (Type: REAL, Questions: 3/3, Time: 8.0s)\n",
      "   â€¢ image89 (Type: REAL, Questions: 3/3, Time: 24.6s)\n",
      "   â€¢ image90 (Type: REAL, Questions: 3/3, Time: 24.1s)\n",
      "   â€¢ image91 (Type: REAL, Questions: 3/3, Time: 3.0s)\n",
      "   â€¢ image92 (Type: REAL, Questions: 3/3, Time: 97.2s)\n",
      "   â€¢ image93 (Type: REAL, Questions: 3/3, Time: 7.9s)\n",
      "   â€¢ image94 (Type: REAL, Questions: 3/3, Time: 8.8s)\n",
      "   â€¢ image95 (Type: REAL, Questions: 3/3, Time: 7.7s)\n",
      "   â€¢ image96 (Type: REAL, Questions: 3/3, Time: 20.5s)\n",
      "   â€¢ image97 (Type: REAL, Questions: 3/3, Time: 25.3s)\n",
      "   â€¢ image98 (Type: REAL, Questions: 3/3, Time: 7.7s)\n",
      "   â€¢ image99 (Type: REAL, Questions: 3/3, Time: 73.9s)\n",
      "   â€¢ image100 (Type: REAL, Questions: 3/3, Time: 8.0s)\n",
      "   â€¢ image101 (Type: REAL, Questions: 3/3, Time: 8.2s)\n",
      "   â€¢ image102 (Type: REAL, Questions: 3/3, Time: 9.0s)\n",
      "   â€¢ image103 (Type: REAL, Questions: 3/3, Time: 9.6s)\n",
      "   â€¢ image104 (Type: REAL, Questions: 3/3, Time: 7.8s)\n",
      "   â€¢ image105 (Type: REAL, Questions: 3/3, Time: 5.1s)\n",
      "   â€¢ image106 (Type: REAL, Questions: 3/3, Time: 18.9s)\n",
      "   â€¢ image107 (Type: REAL, Questions: 3/3, Time: 8.0s)\n",
      "   â€¢ image108 (Type: REAL, Questions: 3/3, Time: 25.5s)\n",
      "   â€¢ image109 (Type: REAL, Questions: 3/3, Time: 25.7s)\n",
      "   â€¢ image110 (Type: REAL, Questions: 3/3, Time: 25.4s)\n",
      "   â€¢ image111 (Type: REAL, Questions: 3/3, Time: 25.3s)\n",
      "   â€¢ image112 (Type: REAL, Questions: 3/3, Time: 9.2s)\n",
      "   â€¢ image113 (Type: REAL, Questions: 3/3, Time: 1.8s)\n",
      "   â€¢ image114 (Type: REAL, Questions: 3/3, Time: 6.9s)\n",
      "   â€¢ image115 (Type: REAL, Questions: 3/3, Time: 9.2s)\n",
      "   â€¢ image116 (Type: REAL, Questions: 3/3, Time: 25.2s)\n",
      "   â€¢ image117 (Type: REAL, Questions: 3/3, Time: 6.6s)\n",
      "   â€¢ image118 (Type: REAL, Questions: 3/3, Time: 7.9s)\n",
      "   â€¢ image119 (Type: REAL, Questions: 3/3, Time: 6.9s)\n",
      "   â€¢ image120 (Type: REAL, Questions: 3/3, Time: 8.6s)\n",
      "   â€¢ image01 (Type: ANIMATED, Questions: 3/3, Time: 1.3s)\n",
      "   â€¢ image02 (Type: ANIMATED, Questions: 3/3, Time: 1.2s)\n",
      "   â€¢ image03 (Type: ANIMATED, Questions: 3/3, Time: 2.1s)\n",
      "   â€¢ image04 (Type: ANIMATED, Questions: 3/3, Time: 3.7s)\n",
      "   â€¢ image05 (Type: ANIMATED, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image06 (Type: ANIMATED, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image07 (Type: ANIMATED, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image08 (Type: ANIMATED, Questions: 3/3, Time: 3.9s)\n",
      "   â€¢ image09 (Type: ANIMATED, Questions: 3/3, Time: 2.7s)\n",
      "   â€¢ image10 (Type: ANIMATED, Questions: 3/3, Time: 96.9s)\n",
      "   â€¢ image11 (Type: ANIMATED, Questions: 3/3, Time: 22.3s)\n",
      "   â€¢ image12 (Type: ANIMATED, Questions: 3/3, Time: 15.7s)\n",
      "   â€¢ image13 (Type: ANIMATED, Questions: 3/3, Time: 24.5s)\n",
      "   â€¢ image14 (Type: ANIMATED, Questions: 3/3, Time: 1.8s)\n",
      "   â€¢ image15 (Type: ANIMATED, Questions: 3/3, Time: 27.9s)\n",
      "   â€¢ image16 (Type: ANIMATED, Questions: 3/3, Time: 27.5s)\n",
      "   â€¢ image17 (Type: ANIMATED, Questions: 3/3, Time: 4.7s)\n",
      "   â€¢ image18 (Type: ANIMATED, Questions: 3/3, Time: 2.6s)\n",
      "   â€¢ image19 (Type: ANIMATED, Questions: 3/3, Time: 3.0s)\n",
      "   â€¢ image20 (Type: ANIMATED, Questions: 3/3, Time: 3.9s)\n",
      "   â€¢ image21 (Type: ANIMATED, Questions: 3/3, Time: 96.4s)\n",
      "   â€¢ image22 (Type: ANIMATED, Questions: 3/3, Time: 15.4s)\n",
      "   â€¢ image23 (Type: ANIMATED, Questions: 3/3, Time: 97.1s)\n",
      "   â€¢ image24 (Type: ANIMATED, Questions: 3/3, Time: 2.1s)\n",
      "   â€¢ image25 (Type: ANIMATED, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image26 (Type: ANIMATED, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image27 (Type: ANIMATED, Questions: 3/3, Time: 2.0s)\n",
      "   â€¢ image28 (Type: ANIMATED, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image29 (Type: ANIMATED, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image30 (Type: ANIMATED, Questions: 3/3, Time: 1.4s)\n",
      "   â€¢ image31 (Type: ANIMATED, Questions: 3/3, Time: 2.0s)\n",
      "   â€¢ image32 (Type: ANIMATED, Questions: 3/3, Time: 1.6s)\n",
      "   â€¢ image33 (Type: ANIMATED, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image34 (Type: ANIMATED, Questions: 3/3, Time: 2.0s)\n",
      "   â€¢ image35 (Type: ANIMATED, Questions: 3/3, Time: 2.1s)\n",
      "   â€¢ image36 (Type: ANIMATED, Questions: 3/3, Time: 2.9s)\n",
      "   â€¢ image37 (Type: ANIMATED, Questions: 3/3, Time: 2.0s)\n",
      "   â€¢ image38 (Type: ANIMATED, Questions: 3/3, Time: 2.1s)\n",
      "   â€¢ image39 (Type: ANIMATED, Questions: 3/3, Time: 2.5s)\n",
      "   â€¢ image40 (Type: ANIMATED, Questions: 3/3, Time: 2.1s)\n",
      "   â€¢ image41 (Type: ANIMATED, Questions: 3/3, Time: 95.4s)\n",
      "   â€¢ image42 (Type: ANIMATED, Questions: 3/3, Time: 27.5s)\n",
      "   â€¢ image43 (Type: ANIMATED, Questions: 3/3, Time: 98.3s)\n",
      "   â€¢ image44 (Type: ANIMATED, Questions: 3/3, Time: 96.3s)\n",
      "   â€¢ image45 (Type: ANIMATED, Questions: 3/3, Time: 95.0s)\n",
      "   â€¢ image46 (Type: ANIMATED, Questions: 3/3, Time: 101.0s)\n",
      "   â€¢ image47 (Type: ANIMATED, Questions: 3/3, Time: 27.8s)\n",
      "   â€¢ image48 (Type: ANIMATED, Questions: 3/3, Time: 27.5s)\n",
      "   â€¢ image49 (Type: ANIMATED, Questions: 3/3, Time: 27.5s)\n",
      "   â€¢ image50 (Type: ANIMATED, Questions: 3/3, Time: 15.3s)\n",
      "   â€¢ image51 (Type: ANIMATED, Questions: 3/3, Time: 100.8s)\n",
      "   â€¢ image52 (Type: ANIMATED, Questions: 3/3, Time: 98.0s)\n",
      "   â€¢ image53 (Type: ANIMATED, Questions: 3/3, Time: 95.9s)\n",
      "   â€¢ image54 (Type: ANIMATED, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image55 (Type: ANIMATED, Questions: 3/3, Time: 1.0s)\n",
      "   â€¢ image56 (Type: ANIMATED, Questions: 3/3, Time: 1.0s)\n",
      "   â€¢ image57 (Type: ANIMATED, Questions: 3/3, Time: 1.2s)\n",
      "   â€¢ image58 (Type: ANIMATED, Questions: 3/3, Time: 97.6s)\n",
      "   â€¢ image59 (Type: ANIMATED, Questions: 3/3, Time: 97.9s)\n",
      "   â€¢ image60 (Type: ANIMATED, Questions: 3/3, Time: 1.5s)\n",
      "   â€¢ image61 (Type: ANIMATED, Questions: 3/3, Time: 2.9s)\n",
      "   â€¢ image62 (Type: ANIMATED, Questions: 3/3, Time: 4.8s)\n",
      "   â€¢ image63 (Type: ANIMATED, Questions: 3/3, Time: 4.9s)\n",
      "   â€¢ image64 (Type: ANIMATED, Questions: 3/3, Time: 0.9s)\n",
      "   â€¢ image65 (Type: ANIMATED, Questions: 3/3, Time: 4.3s)\n",
      "   â€¢ image66 (Type: ANIMATED, Questions: 3/3, Time: 4.5s)\n",
      "   â€¢ image67 (Type: ANIMATED, Questions: 3/3, Time: 6.9s)\n",
      "   â€¢ image68 (Type: ANIMATED, Questions: 3/3, Time: 5.7s)\n",
      "   â€¢ image69 (Type: ANIMATED, Questions: 3/3, Time: 5.9s)\n",
      "   â€¢ image70 (Type: ANIMATED, Questions: 3/3, Time: 6.8s)\n",
      "   â€¢ image71 (Type: ANIMATED, Questions: 3/3, Time: 5.5s)\n",
      "   â€¢ image72 (Type: ANIMATED, Questions: 3/3, Time: 5.5s)\n",
      "   â€¢ image73 (Type: ANIMATED, Questions: 3/3, Time: 6.2s)\n",
      "   â€¢ image74 (Type: ANIMATED, Questions: 3/3, Time: 5.0s)\n",
      "   â€¢ image75 (Type: ANIMATED, Questions: 3/3, Time: 5.4s)\n",
      "   â€¢ image76 (Type: ANIMATED, Questions: 3/3, Time: 6.1s)\n",
      "   â€¢ image77 (Type: ANIMATED, Questions: 3/3, Time: 5.1s)\n",
      "   â€¢ image78 (Type: ANIMATED, Questions: 3/3, Time: 5.2s)\n",
      "   â€¢ image79 (Type: ANIMATED, Questions: 3/3, Time: 6.3s)\n",
      "   â€¢ image80 (Type: ANIMATED, Questions: 3/3, Time: 5.7s)\n",
      "   â€¢ image81 (Type: ANIMATED, Questions: 3/3, Time: 2.6s)\n",
      "   â€¢ image82 (Type: ANIMATED, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image83 (Type: ANIMATED, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image84 (Type: ANIMATED, Questions: 3/3, Time: 97.5s)\n",
      "   â€¢ image85 (Type: ANIMATED, Questions: 3/3, Time: 2.4s)\n",
      "   â€¢ image86 (Type: ANIMATED, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image87 (Type: ANIMATED, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image88 (Type: ANIMATED, Questions: 3/3, Time: 1.3s)\n",
      "   â€¢ image89 (Type: ANIMATED, Questions: 3/3, Time: 1.5s)\n",
      "   â€¢ image90 (Type: ANIMATED, Questions: 3/3, Time: 1.3s)\n",
      "   â€¢ image91 (Type: ANIMATED, Questions: 3/3, Time: 1.6s)\n",
      "   â€¢ image92 (Type: ANIMATED, Questions: 3/3, Time: 1.5s)\n",
      "   â€¢ image93 (Type: ANIMATED, Questions: 3/3, Time: 1.5s)\n",
      "   â€¢ image94 (Type: ANIMATED, Questions: 3/3, Time: 1.6s)\n",
      "   â€¢ image95 (Type: ANIMATED, Questions: 3/3, Time: 1.3s)\n",
      "   â€¢ image96 (Type: ANIMATED, Questions: 3/3, Time: 1.8s)\n",
      "   â€¢ image97 (Type: ANIMATED, Questions: 3/3, Time: 11.4s)\n",
      "   â€¢ image98 (Type: ANIMATED, Questions: 3/3, Time: 3.1s)\n",
      "   â€¢ image99 (Type: ANIMATED, Questions: 3/3, Time: 2.4s)\n",
      "   â€¢ image100 (Type: ANIMATED, Questions: 3/3, Time: 1.2s)\n",
      "   â€¢ image101 (Type: ANIMATED, Questions: 3/3, Time: 4.6s)\n",
      "   â€¢ image102 (Type: ANIMATED, Questions: 3/3, Time: 1.5s)\n",
      "   â€¢ image103 (Type: ANIMATED, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image104 (Type: ANIMATED, Questions: 3/3, Time: 2.4s)\n",
      "   â€¢ image105 (Type: ANIMATED, Questions: 3/3, Time: 1.8s)\n",
      "   â€¢ image106 (Type: ANIMATED, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image107 (Type: ANIMATED, Questions: 3/3, Time: 1.6s)\n",
      "   â€¢ image108 (Type: ANIMATED, Questions: 3/3, Time: 1.3s)\n",
      "   â€¢ image109 (Type: ANIMATED, Questions: 3/3, Time: 4.8s)\n",
      "   â€¢ image110 (Type: ANIMATED, Questions: 3/3, Time: 96.7s)\n",
      "   â€¢ image111 (Type: ANIMATED, Questions: 3/3, Time: 101.2s)\n",
      "   â€¢ image112 (Type: ANIMATED, Questions: 3/3, Time: 97.5s)\n",
      "   â€¢ image113 (Type: ANIMATED, Questions: 3/3, Time: 99.2s)\n",
      "   â€¢ image114 (Type: ANIMATED, Questions: 3/3, Time: 28.1s)\n",
      "   â€¢ image115 (Type: ANIMATED, Questions: 3/3, Time: 99.7s)\n",
      "   â€¢ image116 (Type: ANIMATED, Questions: 3/3, Time: 96.5s)\n",
      "   â€¢ image117 (Type: ANIMATED, Questions: 3/3, Time: 96.3s)\n",
      "   â€¢ image118 (Type: ANIMATED, Questions: 3/3, Time: 97.7s)\n",
      "   â€¢ image119 (Type: ANIMATED, Questions: 3/3, Time: 97.6s)\n",
      "   â€¢ image120 (Type: ANIMATED, Questions: 3/3, Time: 98.4s)\n",
      "   â€¢ image01 (Type: AI_GENERATED, Questions: 3/3, Time: 3.2s)\n",
      "   â€¢ image02 (Type: AI_GENERATED, Questions: 3/3, Time: 3.2s)\n",
      "   â€¢ image03 (Type: AI_GENERATED, Questions: 3/3, Time: 3.2s)\n",
      "   â€¢ image04 (Type: AI_GENERATED, Questions: 3/3, Time: 4.7s)\n",
      "   â€¢ image05 (Type: AI_GENERATED, Questions: 3/3, Time: 2.8s)\n",
      "   â€¢ image06 (Type: AI_GENERATED, Questions: 3/3, Time: 2.8s)\n",
      "   â€¢ image07 (Type: AI_GENERATED, Questions: 3/3, Time: 4.9s)\n",
      "   â€¢ image08 (Type: AI_GENERATED, Questions: 3/3, Time: 5.4s)\n",
      "   â€¢ image09 (Type: AI_GENERATED, Questions: 3/3, Time: 4.7s)\n",
      "   â€¢ image10 (Type: AI_GENERATED, Questions: 3/3, Time: 3.4s)\n",
      "   â€¢ image11 (Type: AI_GENERATED, Questions: 3/3, Time: 4.6s)\n",
      "   â€¢ image12 (Type: AI_GENERATED, Questions: 3/3, Time: 4.0s)\n",
      "   â€¢ image13 (Type: AI_GENERATED, Questions: 3/3, Time: 4.7s)\n",
      "   â€¢ image14 (Type: AI_GENERATED, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image15 (Type: AI_GENERATED, Questions: 3/3, Time: 2.5s)\n",
      "   â€¢ image16 (Type: AI_GENERATED, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image17 (Type: AI_GENERATED, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image18 (Type: AI_GENERATED, Questions: 3/3, Time: 5.1s)\n",
      "   â€¢ image19 (Type: AI_GENERATED, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image20 (Type: AI_GENERATED, Questions: 3/3, Time: 2.1s)\n",
      "   â€¢ image21 (Type: AI_GENERATED, Questions: 3/3, Time: 2.0s)\n",
      "   â€¢ image22 (Type: AI_GENERATED, Questions: 3/3, Time: 2.1s)\n",
      "   â€¢ image23 (Type: AI_GENERATED, Questions: 3/3, Time: 4.9s)\n",
      "   â€¢ image24 (Type: AI_GENERATED, Questions: 3/3, Time: 2.8s)\n",
      "   â€¢ image25 (Type: AI_GENERATED, Questions: 3/3, Time: 2.0s)\n",
      "   â€¢ image26 (Type: AI_GENERATED, Questions: 3/3, Time: 4.8s)\n",
      "   â€¢ image27 (Type: AI_GENERATED, Questions: 3/3, Time: 16.3s)\n",
      "   â€¢ image28 (Type: AI_GENERATED, Questions: 3/3, Time: 2.4s)\n",
      "   â€¢ image29 (Type: AI_GENERATED, Questions: 3/3, Time: 1.5s)\n",
      "   â€¢ image30 (Type: AI_GENERATED, Questions: 3/3, Time: 1.8s)\n",
      "   â€¢ image31 (Type: AI_GENERATED, Questions: 3/3, Time: 1.7s)\n",
      "   â€¢ image32 (Type: AI_GENERATED, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image33 (Type: AI_GENERATED, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image34 (Type: AI_GENERATED, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image35 (Type: AI_GENERATED, Questions: 3/3, Time: 2.0s)\n",
      "   â€¢ image36 (Type: AI_GENERATED, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image37 (Type: AI_GENERATED, Questions: 3/3, Time: 2.1s)\n",
      "   â€¢ image38 (Type: AI_GENERATED, Questions: 3/3, Time: 2.6s)\n",
      "   â€¢ image39 (Type: AI_GENERATED, Questions: 3/3, Time: 16.0s)\n",
      "   â€¢ image40 (Type: AI_GENERATED, Questions: 3/3, Time: 2.4s)\n",
      "   â€¢ image41 (Type: AI_GENERATED, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image42 (Type: AI_GENERATED, Questions: 3/3, Time: 2.4s)\n",
      "   â€¢ image43 (Type: AI_GENERATED, Questions: 3/3, Time: 16.2s)\n",
      "   â€¢ image44 (Type: AI_GENERATED, Questions: 3/3, Time: 3.4s)\n",
      "   â€¢ image45 (Type: AI_GENERATED, Questions: 3/3, Time: 2.1s)\n",
      "   â€¢ image46 (Type: AI_GENERATED, Questions: 3/3, Time: 2.4s)\n",
      "   â€¢ image47 (Type: AI_GENERATED, Questions: 3/3, Time: 2.5s)\n",
      "   â€¢ image48 (Type: AI_GENERATED, Questions: 3/3, Time: 2.4s)\n",
      "   â€¢ image49 (Type: AI_GENERATED, Questions: 3/3, Time: 16.3s)\n",
      "   â€¢ image50 (Type: AI_GENERATED, Questions: 3/3, Time: 14.5s)\n",
      "   â€¢ image51 (Type: AI_GENERATED, Questions: 3/3, Time: 16.8s)\n",
      "   â€¢ image52 (Type: AI_GENERATED, Questions: 3/3, Time: 16.7s)\n",
      "   â€¢ image53 (Type: AI_GENERATED, Questions: 3/3, Time: 16.9s)\n",
      "   â€¢ image54 (Type: AI_GENERATED, Questions: 3/3, Time: 16.9s)\n",
      "   â€¢ image55 (Type: AI_GENERATED, Questions: 3/3, Time: 3.8s)\n",
      "   â€¢ image56 (Type: AI_GENERATED, Questions: 3/3, Time: 3.8s)\n",
      "   â€¢ image57 (Type: AI_GENERATED, Questions: 3/3, Time: 5.3s)\n",
      "   â€¢ image58 (Type: AI_GENERATED, Questions: 3/3, Time: 5.4s)\n",
      "   â€¢ image59 (Type: AI_GENERATED, Questions: 3/3, Time: 4.9s)\n",
      "   â€¢ image60 (Type: AI_GENERATED, Questions: 3/3, Time: 4.7s)\n",
      "   â€¢ image61 (Type: AI_GENERATED, Questions: 3/3, Time: 16.4s)\n",
      "   â€¢ image62 (Type: AI_GENERATED, Questions: 3/3, Time: 16.9s)\n",
      "   â€¢ image63 (Type: AI_GENERATED, Questions: 3/3, Time: 16.8s)\n",
      "   â€¢ image64 (Type: AI_GENERATED, Questions: 3/3, Time: 16.3s)\n",
      "   â€¢ image65 (Type: AI_GENERATED, Questions: 3/3, Time: 16.1s)\n",
      "   â€¢ image66 (Type: AI_GENERATED, Questions: 3/3, Time: 17.5s)\n",
      "   â€¢ image67 (Type: AI_GENERATED, Questions: 3/3, Time: 16.5s)\n",
      "   â€¢ image68 (Type: AI_GENERATED, Questions: 3/3, Time: 16.3s)\n",
      "   â€¢ image69 (Type: AI_GENERATED, Questions: 3/3, Time: 16.4s)\n",
      "   â€¢ image70 (Type: AI_GENERATED, Questions: 3/3, Time: 16.6s)\n",
      "   â€¢ image71 (Type: AI_GENERATED, Questions: 3/3, Time: 1.9s)\n",
      "   â€¢ image72 (Type: AI_GENERATED, Questions: 3/3, Time: 2.1s)\n",
      "   â€¢ image73 (Type: AI_GENERATED, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image74 (Type: AI_GENERATED, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image75 (Type: AI_GENERATED, Questions: 3/3, Time: 2.2s)\n",
      "   â€¢ image76 (Type: AI_GENERATED, Questions: 3/3, Time: 2.1s)\n",
      "   â€¢ image77 (Type: AI_GENERATED, Questions: 3/3, Time: 3.3s)\n",
      "   â€¢ image78 (Type: AI_GENERATED, Questions: 3/3, Time: 3.6s)\n",
      "   â€¢ image79 (Type: AI_GENERATED, Questions: 3/3, Time: 3.1s)\n",
      "   â€¢ image80 (Type: AI_GENERATED, Questions: 3/3, Time: 3.7s)\n",
      "   â€¢ image81 (Type: AI_GENERATED, Questions: 3/3, Time: 3.4s)\n",
      "   â€¢ image82 (Type: AI_GENERATED, Questions: 3/3, Time: 3.3s)\n",
      "   â€¢ image83 (Type: AI_GENERATED, Questions: 3/3, Time: 16.2s)\n",
      "   â€¢ image84 (Type: AI_GENERATED, Questions: 3/3, Time: 3.4s)\n",
      "   â€¢ image85 (Type: AI_GENERATED, Questions: 3/3, Time: 3.0s)\n",
      "   â€¢ image86 (Type: AI_GENERATED, Questions: 3/3, Time: 3.4s)\n",
      "   â€¢ image87 (Type: AI_GENERATED, Questions: 3/3, Time: 16.0s)\n",
      "   â€¢ image88 (Type: AI_GENERATED, Questions: 3/3, Time: 16.0s)\n",
      "   â€¢ image89 (Type: AI_GENERATED, Questions: 3/3, Time: 3.1s)\n",
      "   â€¢ image90 (Type: AI_GENERATED, Questions: 3/3, Time: 3.1s)\n",
      "   â€¢ image91 (Type: AI_GENERATED, Questions: 3/3, Time: 16.0s)\n",
      "   â€¢ image92 (Type: AI_GENERATED, Questions: 3/3, Time: 3.2s)\n",
      "   â€¢ image93 (Type: AI_GENERATED, Questions: 3/3, Time: 3.2s)\n",
      "   â€¢ image94 (Type: AI_GENERATED, Questions: 3/3, Time: 3.3s)\n",
      "   â€¢ image95 (Type: AI_GENERATED, Questions: 3/3, Time: 3.5s)\n",
      "   â€¢ image96 (Type: AI_GENERATED, Questions: 3/3, Time: 16.1s)\n",
      "   â€¢ image97 (Type: AI_GENERATED, Questions: 3/3, Time: 3.1s)\n",
      "   â€¢ image98 (Type: AI_GENERATED, Questions: 3/3, Time: 15.9s)\n",
      "   â€¢ image99 (Type: AI_GENERATED, Questions: 3/3, Time: 3.2s)\n",
      "   â€¢ image100 (Type: AI_GENERATED, Questions: 3/3, Time: 16.5s)\n",
      "   â€¢ image101 (Type: AI_GENERATED, Questions: 3/3, Time: 16.4s)\n",
      "   â€¢ image102 (Type: AI_GENERATED, Questions: 3/3, Time: 3.1s)\n",
      "   â€¢ image103 (Type: AI_GENERATED, Questions: 3/3, Time: 3.0s)\n",
      "   â€¢ image104 (Type: AI_GENERATED, Questions: 3/3, Time: 3.7s)\n",
      "   â€¢ image105 (Type: AI_GENERATED, Questions: 3/3, Time: 3.1s)\n",
      "   â€¢ image106 (Type: AI_GENERATED, Questions: 3/3, Time: 16.0s)\n",
      "   â€¢ image107 (Type: AI_GENERATED, Questions: 3/3, Time: 16.5s)\n",
      "   â€¢ image108 (Type: AI_GENERATED, Questions: 3/3, Time: 3.1s)\n",
      "   â€¢ image109 (Type: AI_GENERATED, Questions: 3/3, Time: 3.1s)\n",
      "   â€¢ image110 (Type: AI_GENERATED, Questions: 3/3, Time: 16.3s)\n",
      "   â€¢ image111 (Type: AI_GENERATED, Questions: 3/3, Time: 3.0s)\n",
      "   â€¢ image112 (Type: AI_GENERATED, Questions: 3/3, Time: 2.9s)\n",
      "   â€¢ image113 (Type: AI_GENERATED, Questions: 3/3, Time: 3.2s)\n",
      "   â€¢ image114 (Type: AI_GENERATED, Questions: 3/3, Time: 3.4s)\n",
      "   â€¢ image115 (Type: AI_GENERATED, Questions: 3/3, Time: 2.3s)\n",
      "   â€¢ image116 (Type: AI_GENERATED, Questions: 3/3, Time: 3.0s)\n",
      "   â€¢ image117 (Type: AI_GENERATED, Questions: 3/3, Time: 3.1s)\n",
      "   â€¢ image118 (Type: AI_GENERATED, Questions: 3/3, Time: 16.6s)\n",
      "   â€¢ image119 (Type: AI_GENERATED, Questions: 3/3, Time: 16.4s)\n",
      "   â€¢ image120 (Type: AI_GENERATED, Questions: 3/3, Time: 13.0s)\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_Qwen2_5VL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09f5ab",
   "metadata": {
    "papermill": {
     "duration": 0.060781,
     "end_time": "2025-07-28T15:07:20.430400",
     "exception": false,
     "start_time": "2025-07-28T15:07:20.369619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7163706,
     "sourceId": 11436696,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python ((op_bench)",
   "language": "python",
   "name": "op_bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5355.688802,
   "end_time": "2025-07-28T15:07:22.890114",
   "environment_variables": {},
   "exception": null,
   "input_path": "/var/scratch/ave303/OP_bench/opa-benchmark-smolvlm2-qwen2-5-vl.ipynb",
   "output_path": "qwen2-5-vl_7b_output.ipynb",
   "parameters": {},
   "start_time": "2025-07-28T13:38:07.201312",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
